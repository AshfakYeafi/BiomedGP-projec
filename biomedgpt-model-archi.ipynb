{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fairseq","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:33:26.642144Z","iopub.execute_input":"2024-10-12T23:33:26.643271Z","iopub.status.idle":"2024-10-12T23:34:32.632047Z","shell.execute_reply.started":"2024-10-12T23:33:26.643222Z","shell.execute_reply":"2024-10-12T23:34:32.630847Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting fairseq\n  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cffi in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.16.0)\nRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from fairseq) (3.0.10)\nCollecting hydra-core<1.1,>=1.0.7 (from fairseq)\n  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\nCollecting omegaconf<2.1 (from fairseq)\n  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from fairseq) (2024.5.15)\nCollecting sacrebleu>=1.4.12 (from fairseq)\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fairseq) (4.66.4)\nCollecting bitarray (from fairseq)\n  Downloading bitarray-2.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nRequirement already satisfied: torchaudio>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.26.4)\nCollecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: PyYAML>=5.1.* in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (6.0.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (4.12.2)\nCollecting portalocker (from sacrebleu>=1.4.12->fairseq)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (5.3.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi->fairseq) (2.22)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->fairseq) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->fairseq) (1.3.0)\nDownloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitarray-2.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nBuilding wheels for collected packages: fairseq, antlr4-python3-runtime\n  Building wheel for fairseq (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=10414537 sha256=41dac7d6d76cfba6af02f82272fc2de944913e01015b801f4b1b58b8b2582cf4\n  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=1968ebcceb8e58c4217d1166c681e8c80f54caeb30b6d4f259814c9c943982cb\n  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\nSuccessfully built fairseq antlr4-python3-runtime\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, sacrebleu, hydra-core, fairseq\nSuccessfully installed antlr4-python3-runtime-4.8 bitarray-2.9.3 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.10.1 sacrebleu-2.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Copyright 2022 The OFA-Sys Team. \n# All rights reserved.\n# This source code is licensed under the Apache 2.0 license \n# found in the LICENSE file in the root directory.\n\nfrom fairseq import options, quantization_utils, tasks, utils\nfrom fairseq.data import iterators\nfrom fairseq.data.plasma_utils import PlasmaStore\nfrom fairseq.dataclass.configs import FairseqConfig\nfrom fairseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom fairseq.distributed import fsdp_enable_wrap, fsdp_wrap, utils as distributed_utils\nfrom fairseq.file_io import PathManager\nfrom fairseq.incremental_decoding_utils import with_incremental_state\nfrom fairseq.modules.fairseq_dropout import FairseqDropout\nfrom fairseq.modules.quant_noise import quant_noise\nfrom fairseq.distributed import fsdp_wrap\nfrom fairseq.models import FairseqEncoder, FairseqEncoderDecoderModel, FairseqIncrementalDecoder, register_model, register_model_architecture\nfrom fairseq.modules import AdaptiveSoftmax, BaseLayer, FairseqDropout, LayerDropModuleList, LayerNorm, SinusoidalPositionalEmbedding, GradMultiply\nfrom fairseq.modules.checkpoint_activations import checkpoint_wrapper\nfrom fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\n\nfrom omegaconf import DictConfig, OmegaConf\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport math\nimport random\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.nn import Parameter\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:34:32.634078Z","iopub.execute_input":"2024-10-12T23:34:32.634419Z","iopub.status.idle":"2024-10-12T23:35:07.147841Z","shell.execute_reply.started":"2024-10-12T23:34:32.634380Z","shell.execute_reply":"2024-10-12T23:35:07.146989Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"@with_incremental_state\nclass MultiheadAttention(nn.Module):\n    \"\"\"Multi-headed attention.\n\n    See \"Attention Is All You Need\" for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        kdim=None,\n        vdim=None,\n        dropout=0.0,\n        bias=True,\n        add_bias_kv=False,\n        add_zero_attn=False,\n        self_attention=False,\n        encoder_decoder_attention=False,\n        q_noise=0.0,\n        qn_block_size=8,\n        scale_factor=2,\n        scale_heads=False\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout_module = FairseqDropout(\n            dropout, module_name=self.__class__.__name__\n        )\n\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n        self.scaling = float(self.head_dim * scale_factor) ** -0.5\n\n        self.self_attention = self_attention\n        self.encoder_decoder_attention = encoder_decoder_attention\n        self.c_attn = nn.Parameter(torch.ones((self.num_heads,)), requires_grad=True) if scale_heads else None\n\n        assert not self.self_attention or self.qkv_same_dim, (\n            \"Self-attention requires query, key and \" \"value to be of the same size\"\n        )\n\n        self.k_proj = quant_noise(\n            nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.v_proj = quant_noise(\n            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.q_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        self.out_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self.reset_parameters()\n\n        self.onnx_trace = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def reset_parameters(self):\n        if self.qkv_same_dim:\n            # Empirically observed the convergence to be much better with\n            # the scaled initialization\n            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n        else:\n            nn.init.xavier_uniform_(self.k_proj.weight)\n            nn.init.xavier_uniform_(self.v_proj.weight)\n            nn.init.xavier_uniform_(self.q_proj.weight)\n\n        nn.init.xavier_uniform_(self.out_proj.weight)\n        if self.out_proj.bias is not None:\n            nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n\n    def forward(\n        self,\n        query,\n        key: Optional[Tensor],\n        value: Optional[Tensor],\n        key_padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        need_weights: bool = True,\n        static_kv: bool = False,\n        attn_mask: Optional[Tensor] = None,\n        self_attn_mask: Optional[Tensor] = None,\n        before_softmax: bool = False,\n        need_head_weights: bool = False,\n        attn_bias: Optional[Tensor] = None,\n        prompt_kv: Optional[Tensor] = None\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_softmax (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        \"\"\"\n        if need_head_weights:\n            need_weights = True\n\n        is_tpu = query.device.type == \"xla\"\n\n        tgt_len, bsz, embed_dim = query.size()\n        src_len = tgt_len\n        assert embed_dim == self.embed_dim, f\"query dim {embed_dim} != {self.embed_dim}\"\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n        if key is not None:\n            src_len, key_bsz, _ = key.size()\n            if not torch.jit.is_scripting():\n                assert key_bsz == bsz\n                assert value is not None\n                assert src_len, bsz == value.shape[:2]\n\n        if (\n            not self.onnx_trace\n            and not is_tpu  # don't use PyTorch version on TPUs\n            and incremental_state is None\n            and not static_kv\n            # A workaround for quantization to work. Otherwise JIT compilation\n            # treats bias in linear module as method.\n            and not torch.jit.is_scripting()\n            and self_attn_mask is None\n            and attn_bias is None\n        ):\n            assert key is not None and value is not None\n            return F.multi_head_attention_forward(\n                query,\n                key,\n                value,\n                self.embed_dim,\n                self.num_heads,\n                torch.empty([0]),\n                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n                self.bias_k,\n                self.bias_v,\n                self.add_zero_attn,\n                self.dropout_module.p,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                self.training or self.dropout_module.apply_during_inference,\n                key_padding_mask,\n                need_weights,\n                attn_mask,\n                use_separate_proj_weight=True,\n                q_proj_weight=self.q_proj.weight,\n                k_proj_weight=self.k_proj.weight,\n                v_proj_weight=self.v_proj.weight,\n            )\n\n        if incremental_state is not None:\n            saved_state = self._get_input_buffer(incremental_state)\n            if saved_state is not None and \"prev_key\" in saved_state:\n                # previous time steps are cached - no need to recompute\n                # key and value if they are static\n                if static_kv:\n                    assert self.encoder_decoder_attention and not self.self_attention\n                    key = value = None\n        else:\n            saved_state = None\n\n        if self.self_attention and self_attn_mask is None:\n            q = self.q_proj(query)\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n        elif self.encoder_decoder_attention:\n            # encoder-decoder attention\n            q = self.q_proj(query)\n            if key is None:\n                assert value is None\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n\n        else:\n            assert key is not None and value is not None\n            q = self.q_proj(query)\n            k = self.k_proj(key)\n            v = self.v_proj(value)\n        q *= self.scaling\n\n        if self.bias_k is not None:\n            assert self.bias_v is not None\n            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n                    ],\n                    dim=1,\n                )\n\n        q = (\n            q.contiguous()\n            .view(tgt_len, bsz * self.num_heads, self.head_dim)\n            .transpose(0, 1)\n        )\n        if k is not None:\n            k = (\n                k.contiguous()\n                .view(-1, bsz * self.num_heads, self.head_dim)\n                .transpose(0, 1)\n            )\n        if v is not None:\n            v = (\n                v.contiguous()\n                .view(-1, bsz * self.num_heads, self.head_dim)\n                .transpose(0, 1)\n            )\n\n        if saved_state is not None:\n            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n            if \"prev_key\" in saved_state:\n                _prev_key = saved_state[\"prev_key\"]\n                assert _prev_key is not None\n                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    k = prev_key\n                else:\n                    assert k is not None\n                    k = torch.cat([prev_key, k], dim=1)\n                src_len = k.size(1)\n            if \"prev_value\" in saved_state:\n                _prev_value = saved_state[\"prev_value\"]\n                assert _prev_value is not None\n                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    v = prev_value\n                else:\n                    assert v is not None\n                    v = torch.cat([prev_value, v], dim=1)\n            prev_key_padding_mask: Optional[Tensor] = None\n            if \"prev_key_padding_mask\" in saved_state:\n                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n            assert k is not None and v is not None\n            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n                key_padding_mask=key_padding_mask,\n                prev_key_padding_mask=prev_key_padding_mask,\n                batch_size=bsz,\n                src_len=k.size(1),\n                static_kv=static_kv,\n            )\n\n            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n        assert k is not None\n        assert k.size(1) == src_len\n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n        if self.add_zero_attn:\n            assert v is not None\n            src_len += 1\n            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n                            key_padding_mask\n                        ),\n                    ],\n                    dim=1,\n                )\n        if prompt_kv is not None:\n            prompt_k, prompt_v = prompt_kv.split(1)\n            prompt_k = prompt_k.squeeze(0).reshape(k.size(0), -1, k.size(2))\n            prompt_v = prompt_v.squeeze(0).reshape(v.size(0), -1, v.size(2))\n            k = torch.cat([prompt_k, k], dim=1)\n            v = torch.cat([prompt_v, v], dim=1)\n        if key_padding_mask is not None:\n            assert key_padding_mask.size(0) == bsz\n            # print(key_padding_mask.size(1), k.size(1))\n            assert key_padding_mask.size(1) == k.size(1)\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, k.size(1), bsz)\n\n        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, k.size(1)]\n\n        if attn_bias is not None:\n            attn_weights[:, :, -src_len:] += attn_bias[:, :, -src_len:]\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.unsqueeze(0)\n            if self.onnx_trace:\n                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n            attn_weights += attn_mask\n\n        if self_attn_mask is not None:\n            self_attn_mask = self_attn_mask.unsqueeze(1).expand(bsz, self.num_heads, tgt_len, k.size(1))\n            attn_weights += self_attn_mask.contiguous().view(bsz * self.num_heads, tgt_len, k.size(1))\n\n        if key_padding_mask is not None:\n            # don't attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, k.size(1))\n            if not is_tpu:\n                attn_weights = attn_weights.masked_fill(\n                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n                    float(\"-inf\"),\n                )\n            else:\n                attn_weights = attn_weights.transpose(0, 2)\n                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n                attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, k.size(1))\n\n        if before_softmax:\n            return attn_weights, v\n\n        attn_weights_float = utils.softmax(\n            attn_weights, dim=-1, onnx_trace=self.onnx_trace\n        )\n        attn_weights = attn_weights_float.type_as(attn_weights)\n        attn_probs = self.dropout_module(attn_weights)\n\n        assert v is not None\n        attn = torch.bmm(attn_probs, v)\n        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n        if self.onnx_trace and attn.size(1) == 1:\n            # when ONNX tracing a single decoder step (sequence length == 1)\n            # the transpose is a no-op copy before view, thus unnecessary\n            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n        else:\n            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n        if self.c_attn is not None:\n            attn = attn.view(tgt_len, bsz, self.num_heads, self.head_dim)\n            attn = torch.einsum('tbhd,h->tbhd', attn, self.c_attn)\n            attn = attn.reshape(tgt_len, bsz, self.embed_dim)\n        attn = self.out_proj(attn)\n        attn_weights: Optional[Tensor] = None\n        if need_weights:\n            attn_weights = attn_weights_float.view(\n                bsz, self.num_heads, tgt_len, k.size(1)\n            ).transpose(1, 0)\n            if not need_head_weights:\n                # average attention weights over heads\n                attn_weights = attn_weights.mean(dim=0)\n\n        return attn, attn_weights\n\n    @staticmethod\n    def _append_prev_key_padding_mask(\n        key_padding_mask: Optional[Tensor],\n        prev_key_padding_mask: Optional[Tensor],\n        batch_size: int,\n        src_len: int,\n        static_kv: bool,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_key_padding_mask is not None and static_kv:\n            new_key_padding_mask = prev_key_padding_mask\n        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n            )\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_key_padding_mask is not None:\n            if src_len > prev_key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n                    device=prev_key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat(\n                    [prev_key_padding_mask.float(), filler.float()], dim=1\n                )\n            else:\n                new_key_padding_mask = prev_key_padding_mask.float()\n        elif key_padding_mask is not None:\n            if src_len > key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - key_padding_mask.size(1)),\n                    device=key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat(\n                    [filler.float(), key_padding_mask.float()], dim=1\n                )\n            else:\n                new_key_padding_mask = key_padding_mask.float()\n        else:\n            new_key_padding_mask = prev_key_padding_mask\n        return new_key_padding_mask\n\n    @torch.jit.export\n    def reorder_incremental_state(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        new_order: Tensor,\n    ):\n        \"\"\"Reorder buffered internal state (for incremental generation).\"\"\"\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            for k in input_buffer.keys():\n                input_buffer_k = input_buffer[k]\n                if input_buffer_k is not None:\n                    if self.encoder_decoder_attention and input_buffer_k.size(\n                        0\n                    ) == new_order.size(0):\n                        break\n                    input_buffer[k] = input_buffer_k.index_select(0, new_order)\n            incremental_state = self._set_input_buffer(incremental_state, input_buffer)\n        return incremental_state\n\n    def _get_input_buffer(\n        self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n    ) -> Dict[str, Optional[Tensor]]:\n        result = self.get_incremental_state(incremental_state, \"attn_state\")\n        if result is not None:\n            return result\n        else:\n            empty_result: Dict[str, Optional[Tensor]] = {}\n            return empty_result\n\n    def _set_input_buffer(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        buffer: Dict[str, Optional[Tensor]],\n    ):\n        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n\n    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n        return attn_weights\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        prefix = name + \".\" if name != \"\" else \"\"\n        items_to_add = {}\n        keys_to_remove = []\n        for k in state_dict.keys():\n            if k.endswith(prefix + \"in_proj_weight\"):\n                # in_proj_weight used to be q + k + v with same dimensions\n                dim = int(state_dict[k].shape[0] / 3)\n                items_to_add[prefix + \"q_proj.weight\"] = state_dict[k][:dim]\n                items_to_add[prefix + \"k_proj.weight\"] = state_dict[k][dim : 2 * dim]\n                items_to_add[prefix + \"v_proj.weight\"] = state_dict[k][2 * dim :]\n\n                keys_to_remove.append(k)\n\n                k_bias = prefix + \"in_proj_bias\"\n                if k_bias in state_dict.keys():\n                    dim = int(state_dict[k].shape[0] / 3)\n                    items_to_add[prefix + \"q_proj.bias\"] = state_dict[k_bias][:dim]\n                    items_to_add[prefix + \"k_proj.bias\"] = state_dict[k_bias][\n                        dim : 2 * dim\n                    ]\n                    items_to_add[prefix + \"v_proj.bias\"] = state_dict[k_bias][2 * dim :]\n\n                    keys_to_remove.append(prefix + \"in_proj_bias\")\n\n        for k in keys_to_remove:\n            del state_dict[k]\n\n        for key, value in items_to_add.items():\n            state_dict[key] = value\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:35:07.149223Z","iopub.execute_input":"2024-10-12T23:35:07.149803Z","iopub.status.idle":"2024-10-12T23:35:07.228905Z","shell.execute_reply.started":"2024-10-12T23:35:07.149766Z","shell.execute_reply":"2024-10-12T23:35:07.227828Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"\n    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n    argument.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (1, x.shape[1], 1)\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\ndef init_bert_weights(module):\n    \"\"\"Initialize the weights.\"\"\"\n    if isinstance(module, (nn.Linear, nn.Embedding)):\n        # std defaults to 0.02, this might need to be changed\n        module.weight.data.normal_(mean=0.0, std=0.02)\n    elif isinstance(module, nn.LayerNorm):\n        module.bias.data.zero_()\n        module.weight.data.fill_(1.0)\n    if isinstance(module, nn.Linear) and module.bias is not None:\n        module.bias.data.zero_()\n\n\nclass Adapter_Layer(torch.nn.Module):\n    def __init__(self,\n                 d_model=None,\n                 down_size=None,\n                 dropout=0.0,\n                 init_option=\"bert\",\n                 adapter_scalar=\"1.0\"):\n        super().__init__()\n        self.n_embd = d_model\n        self.down_size = down_size\n\n\n        if adapter_scalar == \"learnable_scalar\":\n            self.scale = nn.Parameter(torch.ones(1))\n        else:\n            self.scale = float(adapter_scalar)\n\n        self.down_proj = nn.Linear(self.n_embd, self.down_size)\n        self.non_linear_func = nn.ReLU()\n        self.up_proj = nn.Linear(self.down_size, self.n_embd)\n\n        self.dropout = dropout\n        if init_option == \"bert\":\n            self.apply(init_bert_weights)\n        elif init_option == \"lora\":\n            with torch.no_grad():\n                nn.init.kaiming_uniform_(self.down_proj.weight, a=math.sqrt(5))\n                nn.init.zeros_(self.up_proj.weight)\n                nn.init.zeros_(self.down_proj.bias)\n                nn.init.zeros_(self.up_proj.bias)\n\n    def forward(self, x, add_residual=True, residual=None):\n        residual = x if residual is None else residual\n\n        down = self.down_proj(x)\n        down = self.non_linear_func(down)\n        down = nn.functional.dropout(down, p=self.dropout, training=self.training)\n        up = self.up_proj(down)\n        up = up * self.scale\n        if add_residual:\n            output = up + residual\n        else:\n            output = up\n\n        return output\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass TransformerEncoderLayer(nn.Module):\n    \"\"\"Encoder layer block.\n\n    In the original paper each operation (multi-head attention or FFN) is\n    postprocessed with: `dropout -> add residual -> layernorm`. In the\n    tensor2tensor code they suggest that learning is more robust when\n    preprocessing each layer with layernorm and postprocessing with:\n    `dropout -> add residual`. We default to the approach in the paper, but the\n    tensor2tensor approach can be enabled by setting\n    *args.encoder_normalize_before* to ``True``.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n    \"\"\"\n\n    def __init__(self, args, drop_path_rate=0.0, use_adapter=False, adapter_dim=200):\n        super().__init__()\n        self.args = args\n        self.use_adapter = use_adapter\n        self.embed_dim = args.encoder_embed_dim\n        if use_adapter:\n            self.adapter = Adapter_Layer(d_model=self.embed_dim, down_size=adapter_dim)\n        self.quant_noise = getattr(args, 'quant_noise_pq', 0)\n        self.quant_noise_block_size = getattr(args, 'quant_noise_pq_block_size', 8) or 8\n        self.self_attn = self.build_self_attention(self.embed_dim, args)\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.dropout_module = FairseqDropout(\n            args.dropout, module_name=self.__class__.__name__\n        )\n        self.activation_fn = utils.get_activation_fn(\n            activation=getattr(args, 'activation_fn', 'relu') or \"relu\"\n        )\n        activation_dropout_p = getattr(args, \"activation_dropout\", 0) or 0\n        if activation_dropout_p == 0:\n            # for backwards compatibility with models that use args.relu_dropout\n            activation_dropout_p = getattr(args, \"relu_dropout\", 0) or 0\n        self.activation_dropout_module = FairseqDropout(\n            float(activation_dropout_p), module_name=self.__class__.__name__\n        )\n        self.normalize_before = args.encoder_normalize_before\n        self.fc1 = self.build_fc1(\n            self.embed_dim,\n            args.encoder_ffn_embed_dim,\n            self.quant_noise,\n            self.quant_noise_block_size,\n        )\n        self.fc2 = self.build_fc2(\n            args.encoder_ffn_embed_dim,\n            self.embed_dim,\n            self.quant_noise,\n            self.quant_noise_block_size,\n        )\n\n        self.attn_ln = LayerNorm(self.embed_dim) if getattr(args, 'scale_attn', False) else None\n        self.nh = self.self_attn.num_heads\n        self.head_dim = self.self_attn.head_dim\n\n        self.ffn_layernorm = LayerNorm(args.encoder_ffn_embed_dim) if getattr(args, 'scale_fc', False) else None\n        self.w_resid = nn.Parameter(torch.ones(self.embed_dim, ), requires_grad=True) if getattr(args, 'scale_resids', False) else None\n\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(\n            nn.Linear(input_dim, output_dim), p=q_noise, block_size=qn_block_size\n        )\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(\n            nn.Linear(input_dim, output_dim), p=q_noise, block_size=qn_block_size\n        )\n\n    def build_self_attention(self, embed_dim, args):\n        return MultiheadAttention(\n            embed_dim,\n            args.encoder_attention_heads,\n            dropout=args.attention_dropout,\n            self_attention=True,\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n            scale_factor=args.attn_scale_factor,\n            scale_heads=getattr(args, 'scale_heads', False)\n        )\n\n    def residual_connection(self, x, residual):\n        return residual + self.drop_path(x)\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        \"\"\"\n        Rename layer norm states from `...layer_norms.0.weight` to\n        `...self_attn_layer_norm.weight` and `...layer_norms.1.weight` to\n        `...final_layer_norm.weight`\n        \"\"\"\n        layer_norm_map = {\"0\": \"self_attn_layer_norm\", \"1\": \"final_layer_norm\"}\n        for old, new in layer_norm_map.items():\n            for m in (\"weight\", \"bias\"):\n                k = \"{}.layer_norms.{}.{}\".format(name, old, m)\n                if k in state_dict:\n                    state_dict[\"{}.{}.{}\".format(name, new, m)] = state_dict[k]\n                    del state_dict[k]\n                if \"{}.{}.{}\".format(name, new, m) not in state_dict and \"{}.{}\".format(new, m) in self.state_dict():\n                    state_dict[\n                        \"{}.{}.{}\".format(name, new, m)\n                    ] = self.state_dict()[\"{}.{}\".format(new, m)]\n\n        prefix = name + \".\" if name != \"\" else \"\"\n        for param_name, param_tensor in self.state_dict().items():\n            if (prefix + param_name) not in state_dict:\n                state_dict[prefix + param_name] = self.state_dict()[param_name]\n\n    def forward(\n        self,\n        x,\n        encoder_padding_mask: Optional[Tensor],\n        attn_mask: Optional[Tensor] = None,\n        self_attn_bias: Optional[Tensor] = None,\n        prompt_kv: Optional[Tensor] = None\n    ):\n        \"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, seq_len)` where padding elements are indicated by ``1``.\n            attn_mask (ByteTensor): binary tensor of shape `(tgt_len, src_len)`,\n                where `tgt_len` is the length of output and `src_len` is the\n                length of input, though here both are equal to `seq_len`.\n                `attn_mask[tgt_i, src_j] = 1` means that when calculating the\n                embedding for `tgt_i`, we exclude (mask out) `src_j`. This is\n                useful for strided self-attention.\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"\n        # anything in original attn_mask = 1, becomes -1e8\n        # anything in original attn_mask = 0, becomes 0\n        # Note that we cannot use -inf here, because at some edge cases,\n        # the attention weight (before softmax) for some padded element in query\n        # will become -inf, which results in NaN in model parameters\n        if attn_mask is not None:\n            attn_mask = attn_mask.masked_fill(\n                attn_mask.to(torch.bool),\n                -1e8 if x.dtype == torch.float32 else -1e4\n            )\n\n        residual = x\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n        x, _ = self.self_attn(\n            query=x,\n            key=x,\n            value=x,\n            key_padding_mask=encoder_padding_mask,\n            need_weights=False,\n            attn_mask=attn_mask,\n            attn_bias=self_attn_bias,\n            prompt_kv=prompt_kv\n        )\n        if self.attn_ln is not None:\n            x = self.attn_ln(x)\n        x = self.dropout_module(x)\n        x = self.residual_connection(x, residual)\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n        x = self.activation_fn(self.fc1(x))\n        x = self.activation_dropout_module(x)\n        if self.ffn_layernorm is not None:\n            x = self.ffn_layernorm(x)\n        x = self.fc2(x)\n        x = self.dropout_module(x)\n        if self.use_adapter:\n            x = self.adapter(x)\n        if self.w_resid is not None:\n            residual = torch.mul(self.w_resid, residual)\n        x = self.residual_connection(x, residual)\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n        return x\n\n\nclass TransformerDecoderLayer(nn.Module):\n    \"\"\"Decoder layer block.\n\n    In the original paper each operation (multi-head attention, encoder\n    attention or FFN) is postprocessed with: `dropout -> add residual ->\n    layernorm`. In the tensor2tensor code they suggest that learning is more\n    robust when preprocessing each layer with layernorm and postprocessing with:\n    `dropout -> add residual`. We default to the approach in the paper, but the\n    tensor2tensor approach can be enabled by setting\n    *args.decoder_normalize_before* to ``True``.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n            (default: False).\n    \"\"\"\n\n    def __init__(\n        self, args, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False, \\\n            drop_path_rate=0.0, use_adapter=False, adapter_dim=200):\n        super().__init__()\n        self.embed_dim = args.decoder_embed_dim\n        self.use_adapter = use_adapter\n        if use_adapter == True:\n            self.adapter = Adapter_Layer(d_model=self.embed_dim, down_size=adapter_dim)\n        self.dropout_module = FairseqDropout(\n            args.dropout, module_name=self.__class__.__name__\n        )\n        self.quant_noise = getattr(args, \"quant_noise_pq\", 0)\n        self.quant_noise_block_size = getattr(args, \"quant_noise_pq_block_size\", 8)\n\n        self.cross_self_attention = getattr(args, \"cross_self_attention\", False)\n\n        self.self_attn = self.build_self_attention(\n            self.embed_dim,\n            args,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n        )\n        self.self_attn_ln = LayerNorm(self.embed_dim) if getattr(args, 'scale_attn', False) else None\n        self.cross_attn_ln = LayerNorm(self.embed_dim) if getattr(args, 'scale_attn', False) else None\n        self.nh = self.self_attn.num_heads\n        self.head_dim = self.self_attn.head_dim\n\n        self.activation_fn = utils.get_activation_fn(\n            activation=str(args.activation_fn)\n            if getattr(args, \"activation_fn\", None) is not None\n            else \"relu\"\n        )\n        activation_dropout_p = getattr(args, \"activation_dropout\", 0) or 0\n        if activation_dropout_p == 0:\n            # for backwards compatibility with models that use args.relu_dropout\n            activation_dropout_p = getattr(args, \"relu_dropout\", 0) or 0\n        self.activation_dropout_module = FairseqDropout(\n            float(activation_dropout_p), module_name=self.__class__.__name__\n        )\n        self.normalize_before = args.decoder_normalize_before\n\n        # use layerNorm rather than FusedLayerNorm for exporting.\n        # char_inputs can be used to determint this.\n        # TODO  remove this once we update apex with the fix\n        export = getattr(args, \"char_inputs\", False)\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        if no_encoder_attn:\n            self.encoder_attn = None\n            self.encoder_attn_layer_norm = None\n        else:\n            self.encoder_attn = self.build_encoder_attention(self.embed_dim, args)\n            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        self.ffn_layernorm = LayerNorm(args.decoder_ffn_embed_dim) if getattr(args, 'scale_fc', False) else None\n        self.w_resid = nn.Parameter(torch.ones(self.embed_dim, ), requires_grad=True) if getattr(args, 'scale_resids', False) else None\n\n        self.fc1 = self.build_fc1(\n            self.embed_dim,\n            args.decoder_ffn_embed_dim,\n            self.quant_noise,\n            self.quant_noise_block_size,\n        )\n        self.fc2 = self.build_fc2(\n            args.decoder_ffn_embed_dim,\n            self.embed_dim,\n            self.quant_noise,\n            self.quant_noise_block_size,\n        )\n\n        self.final_layer_norm = LayerNorm(self.embed_dim, export=export)\n        self.need_attn = True\n\n        self.onnx_trace = False\n\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_self_attention(\n        self, embed_dim, args, add_bias_kv=False, add_zero_attn=False\n    ):\n        return MultiheadAttention(\n            embed_dim,\n            args.decoder_attention_heads,\n            dropout=args.attention_dropout,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            self_attention=not getattr(args, \"cross_self_attention\", False),\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n            scale_factor=args.attn_scale_factor,\n            scale_heads=getattr(args, 'scale_heads', False)\n        )\n\n    def build_encoder_attention(self, embed_dim, args):\n        return MultiheadAttention(\n            embed_dim,\n            args.decoder_attention_heads,\n            kdim=getattr(args, \"encoder_embed_dim\", None),\n            vdim=getattr(args, \"encoder_embed_dim\", None),\n            dropout=args.attention_dropout,\n            encoder_decoder_attention=True,\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n            scale_factor=args.attn_scale_factor,\n            scale_heads=getattr(args, 'scale_heads', False)\n        )\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def residual_connection(self, x, residual):\n        return residual + self.drop_path(x)\n\n    def forward(\n        self,\n        x,\n        encoder_out: Optional[torch.Tensor] = None,\n        encoder_padding_mask: Optional[torch.Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        prev_self_attn_state: Optional[List[torch.Tensor]] = None,\n        prev_attn_state: Optional[List[torch.Tensor]] = None,\n        self_attn_mask: Optional[torch.Tensor] = None,\n        self_attn_padding_mask: Optional[torch.Tensor] = None,\n        need_attn: bool = False,\n        need_head_weights: bool = False,\n        self_attn_bias: Optional[Tensor] = None,\n        cross_attn_bias: Optional[Tensor] = None,\n        prompt_kv: Optional[Tensor] = None\n    ):\n        \"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor, optional): binary\n                ByteTensor of shape `(batch, src_len)` where padding\n                elements are indicated by ``1``.\n            need_attn (bool, optional): return attention weights\n            need_head_weights (bool, optional): return attention weights\n                for each head (default: return average over heads).\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"\n        if need_head_weights:\n            need_attn = True\n\n        residual = x\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n        if prev_self_attn_state is not None:\n            prev_key, prev_value = prev_self_attn_state[:2]\n            saved_state: Dict[str, Optional[Tensor]] = {\n                \"prev_key\": prev_key,\n                \"prev_value\": prev_value,\n            }\n            if len(prev_self_attn_state) >= 3:\n                saved_state[\"prev_key_padding_mask\"] = prev_self_attn_state[2]\n            assert incremental_state is not None\n            self.self_attn._set_input_buffer(incremental_state, saved_state)\n        _self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)\n        if self.cross_self_attention and not (\n            incremental_state is not None\n            and _self_attn_input_buffer is not None\n            and \"prev_key\" in _self_attn_input_buffer\n        ):\n            if self_attn_mask is not None:\n                assert encoder_out is not None\n                self_attn_mask = torch.cat(\n                    (x.new_zeros(x.size(0), encoder_out.size(0)), self_attn_mask), dim=1\n                )\n            if self_attn_padding_mask is not None:\n                if encoder_padding_mask is None:\n                    assert encoder_out is not None\n                    encoder_padding_mask = self_attn_padding_mask.new_zeros(\n                        encoder_out.size(1), encoder_out.size(0)\n                    )\n                self_attn_padding_mask = torch.cat(\n                    (encoder_padding_mask, self_attn_padding_mask), dim=1\n                )\n            assert encoder_out is not None\n            y = torch.cat((encoder_out, x), dim=0)\n        else:\n            y = x\n\n        x, attn = self.self_attn(\n            query=x,\n            key=y,\n            value=y,\n            key_padding_mask=self_attn_padding_mask,\n            incremental_state=incremental_state,\n            need_weights=False,\n            attn_mask=self_attn_mask,\n            attn_bias=self_attn_bias,\n            prompt_kv=prompt_kv\n        )\n        if self.self_attn_ln is not None:\n            x = self.self_attn_ln(x)\n        x = self.dropout_module(x)\n        x = self.residual_connection(x, residual)\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        if self.encoder_attn is not None and encoder_out is not None:\n            residual = x\n            if self.normalize_before:\n                x = self.encoder_attn_layer_norm(x)\n            if prev_attn_state is not None:\n                prev_key, prev_value = prev_attn_state[:2]\n                saved_state: Dict[str, Optional[Tensor]] = {\n                    \"prev_key\": prev_key,\n                    \"prev_value\": prev_value,\n                }\n                if len(prev_attn_state) >= 3:\n                    saved_state[\"prev_key_padding_mask\"] = prev_attn_state[2]\n                assert incremental_state is not None\n                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n\n            x, attn = self.encoder_attn(\n                query=x,\n                key=encoder_out,\n                value=encoder_out,\n                key_padding_mask=encoder_padding_mask,\n                incremental_state=incremental_state,\n                static_kv=True,\n                need_weights=need_attn or (not self.training and self.need_attn),\n                need_head_weights=need_head_weights,\n                attn_bias=cross_attn_bias\n            )\n            if self.cross_attn_ln is not None:\n                x = self.cross_attn_ln(x)\n            x = self.dropout_module(x)\n            x = self.residual_connection(x, residual)\n            if not self.normalize_before:\n                x = self.encoder_attn_layer_norm(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n\n        x = self.activation_fn(self.fc1(x))\n        x = self.activation_dropout_module(x)\n        if self.ffn_layernorm is not None:\n            x = self.ffn_layernorm(x)\n        x = self.fc2(x)\n        x = self.dropout_module(x)\n        if self.use_adapter == True:\n            x = self.adapter(x)\n        if self.w_resid is not None:\n            residual = torch.mul(self.w_resid, residual)\n        x = self.residual_connection(x, residual)\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n        if self.onnx_trace and incremental_state is not None:\n            saved_state = self.self_attn._get_input_buffer(incremental_state)\n            assert saved_state is not None\n            if self_attn_padding_mask is not None:\n                self_attn_state = [\n                    saved_state[\"prev_key\"],\n                    saved_state[\"prev_value\"],\n                    saved_state[\"prev_key_padding_mask\"],\n                ]\n            else:\n                self_attn_state = [saved_state[\"prev_key\"], saved_state[\"prev_value\"]]\n            return x, attn, self_attn_state\n        return x, attn, None\n\n    def make_generation_fast_(self, need_attn: bool = False, **kwargs):\n        self.need_attn = need_attn\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        \"\"\"\n        Rename layer norm states from `...layer_norms.0.weight` to\n        `...self_attn_layer_norm.weight` and `...layer_norms.1.weight` to\n        `...final_layer_norm.weight`\n        \"\"\"\n        # update layer norms\n        layer_norm_map = {\n            \"0\": \"self_attn_layer_norm\",\n            \"1\": \"encoder_attn_layer_norm\",\n            \"2\": \"final_layer_norm\",\n        }\n        for old, new in layer_norm_map.items():\n            for m in (\"weight\", \"bias\"):\n                k = \"{}.layer_norms.{}.{}\".format(name, old, m)\n                if k in state_dict:\n                    state_dict[\n                        \"{}.{}.{}\".format(name, new, m)\n                    ] = state_dict[k]\n                    del state_dict[k]\n                if \"{}.{}.{}\".format(name, new, m) not in state_dict and \"{}.{}\".format(new, m) in self.state_dict():\n                    state_dict[\n                        \"{}.{}.{}\".format(name, new, m)\n                    ] = self.state_dict()[\"{}.{}\".format(new, m)]\n\n        prefix = name + \".\" if name != \"\" else \"\"\n        for param_name, param_tensor in self.state_dict().items():\n            if (prefix + param_name) not in state_dict:\n                state_dict[prefix + param_name] = self.state_dict()[param_name]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:35:07.231656Z","iopub.execute_input":"2024-10-12T23:35:07.232442Z","iopub.status.idle":"2024-10-12T23:35:07.327110Z","shell.execute_reply.started":"2024-10-12T23:35:07.232358Z","shell.execute_reply":"2024-10-12T23:35:07.326167Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def drop_path(x, drop_prob: float = 0., training: bool = False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a.sh different form of dropout in a.sh separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a.sh layer name and use\n    'survival rate' as the argument.\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        assert False\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, drop_path_rate=0.0):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = identity + self.drop_path(out)\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, layers, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None, drop_path_rate=0.0):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(Bottleneck, 64, layers[0], drop_path_rate=drop_path_rate)\n        self.layer2 = self._make_layer(Bottleneck, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0], drop_path_rate=drop_path_rate)\n        self.layer3 = self._make_layer(Bottleneck, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1], drop_path_rate=drop_path_rate)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.SyncBatchNorm, nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False, drop_path_rate=0.0):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, blocks)]\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer, drop_path_rate=dpr[i]))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x):\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:35:07.328831Z","iopub.execute_input":"2024-10-12T23:35:07.329432Z","iopub.status.idle":"2024-10-12T23:35:07.370415Z","shell.execute_reply.started":"2024-10-12T23:35:07.329384Z","shell.execute_reply":"2024-10-12T23:35:07.369458Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Modified from detectron2: https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py#L13\n\n\nclass FrozenBatchNorm2d(nn.Module):\n    \"\"\"\n    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n\n    It contains non-trainable buffers called\n    \"weight\" and \"bias\", \"running_mean\", \"running_var\",\n    initialized to perform identity transformation.\n\n    The pre-trained backbone models from Caffe2 only contain \"weight\" and \"bias\",\n    which are computed from the original four parameters of BN.\n    The affine transform `x * weight + bias` will perform the equivalent\n    computation of `(x - running_mean) / sqrt(running_var) * weight + bias`.\n    When loading a backbone model from Caffe2, \"running_mean\" and \"running_var\"\n    will be left unchanged as identity transformation.\n\n    Other pre-trained backbone models may contain all 4 parameters.\n\n    The forward is implemented by `F.batch_norm(..., training=False)`.\n    \"\"\"\n\n    def __init__(self, num_features, eps=1e-5):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.register_buffer(\"weight\", torch.ones(num_features))\n        self.register_buffer(\"bias\", torch.zeros(num_features))\n        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n        self.register_buffer(\"running_var\", torch.ones(num_features) - eps)\n\n    def forward(self, x):\n        if x.requires_grad:\n            # When gradients are needed, F.batch_norm will use extra memory\n            # because its backward op computes gradients for weight/bias as well.\n            scale = self.weight * (self.running_var + self.eps).rsqrt()\n            bias = self.bias - self.running_mean * scale\n            scale = scale.reshape(1, -1, 1, 1)\n            bias = bias.reshape(1, -1, 1, 1)\n            out_dtype = x.dtype  # may be half\n            return x * scale.to(out_dtype) + bias.to(out_dtype)\n        else:\n            # When gradients are not needed, F.batch_norm is a single fused op\n            # and provide more optimization opportunities.\n            return F.batch_norm(\n                x,\n                self.running_mean,\n                self.running_var,\n                self.weight,\n                self.bias,\n                training=False,\n                eps=self.eps,\n            )\n\n    def _load_from_state_dict(\n        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n    ):\n        num_batches_tracked_key = prefix + 'num_batches_tracked'\n        if num_batches_tracked_key in state_dict:\n            del state_dict[num_batches_tracked_key]\n        version = local_metadata.get(\"version\", None)\n\n        if version is None or version < 2:\n            # No running_mean/var in early versions\n            # This will silent the warnings\n            if prefix + \"running_mean\" not in state_dict:\n                state_dict[prefix + \"running_mean\"] = torch.zeros_like(self.running_mean)\n            if prefix + \"running_var\" not in state_dict:\n                state_dict[prefix + \"running_var\"] = torch.ones_like(self.running_var)\n\n        super()._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n        )\n\n    def __repr__(self):\n        return \"FrozenBatchNorm2d(num_features={}, eps={})\".format(self.num_features, self.eps)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:35:07.371511Z","iopub.execute_input":"2024-10-12T23:35:07.372152Z","iopub.status.idle":"2024-10-12T23:35:07.386324Z","shell.execute_reply.started":"2024-10-12T23:35:07.372118Z","shell.execute_reply":"2024-10-12T23:35:07.385463Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copyright 2022 The OFA-Sys Team. \n# All rights reserved.\n# This source code is licensed under the Apache 2.0 license \n# found in the LICENSE file in the root directory.\n\n\n\n\n\nDEFAULT_MAX_SOURCE_POSITIONS = 1024\nDEFAULT_MAX_TARGET_POSITIONS = 1024\n\n\nDEFAULT_MIN_PARAMS_TO_WRAP = int(1e8)\n\n\ndef BatchNorm2d(out_chan, momentum=0.1, eps=1e-3):\n    return nn.SyncBatchNorm.convert_sync_batchnorm(\n        nn.BatchNorm2d(out_chan, momentum=momentum, eps=eps)\n    )\n\n\ndef make_token_bucket_position(bucket_size, max_position=DEFAULT_MAX_SOURCE_POSITIONS):\n    context_pos = torch.arange(max_position, dtype=torch.long)[:, None]\n    memory_pos = torch.arange(max_position, dtype=torch.long)[None, :]\n    relative_pos = context_pos - memory_pos\n    sign = torch.sign(relative_pos)\n    mid = bucket_size // 2\n    abs_pos = torch.where((relative_pos<mid) & (relative_pos > -mid), mid-1, torch.abs(relative_pos))\n    log_pos = torch.ceil(torch.log(abs_pos/mid)/math.log((max_position-1)/mid) * (mid-1)) + mid\n    log_pos = log_pos.int()\n    bucket_pos = torch.where(abs_pos.le(mid), relative_pos, log_pos*sign).long()\n    return bucket_pos + bucket_size - 1\n\n\ndef make_image_bucket_position(bucket_size, num_relative_distance):\n    coords_h = torch.arange(bucket_size)\n    coords_w = torch.arange(bucket_size)\n    coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n    relative_coords[:, :, 0] += bucket_size - 1  # shift to start from 0\n    relative_coords[:, :, 1] += bucket_size - 1\n    relative_coords[:, :, 0] *= 2 * bucket_size - 1\n    relative_position_index = torch.zeros(size=(bucket_size * bucket_size + 1,) * 2, dtype=relative_coords.dtype)\n    relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n    relative_position_index[0, 0:] = num_relative_distance - 3\n    relative_position_index[0:, 0] = num_relative_distance - 2\n    relative_position_index[0, 0] = num_relative_distance - 1\n    return relative_position_index\n\n\nclass PromptEncoder(torch.nn.Module):\n    r\"\"\"\n    Prompt encoder to generate prompts, including prompt, prefix, instance and instruction\n    \"\"\"\n\n    def __init__(\n            self,\n            type,\n            length,\n            projection,\n            embed_dim,\n            proj_dim,\n            layers,\n            vocab_size):\n        super().__init__()\n        self.prefix_projection = projection\n\n        if type == \"prefix\":\n            layers = layers\n            prompt_vocab_size = length\n\n        if self.prefix_projection:\n            self.embedding = torch.nn.Embedding(prompt_vocab_size, embed_dim)\n            self.trans = torch.nn.Sequential(\n                torch.nn.Linear(embed_dim, proj_dim),\n                torch.nn.ReLU(),\n                torch.nn.Linear(proj_dim, layers * 2 * embed_dim)\n            )\n        else:\n            if type == \"prefix\":\n                self.embedding = torch.nn.Embedding(\n                    prompt_vocab_size, layers * 2 * embed_dim)\n\n    def forward(self, prefix: torch.Tensor):\n        if self.prefix_projection:\n            prefix_tokens = self.embedding(prefix)\n            past_key_values = self.trans(prefix_tokens)\n        else:\n            past_key_values = self.embedding(prefix)\n        return past_key_values\n\nclass TransformerEncoder(FairseqEncoder):\n    \"\"\"\n    Transformer encoder consisting of *args.encoder_layers* layers. Each layer\n    is a :class:`TransformerEncoderLayer`.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): encoding dictionary\n        embed_tokens (torch.nn.Embedding): input embedding\n    \"\"\"\n\n    def __init__(self, args, dictionary, embed_tokens):\n        self.args = args\n        super().__init__(dictionary)\n        self.register_buffer(\"version\", torch.Tensor([3]))\n  \n        if getattr(args, \"encoder_prompt\", False):\n            self.encoder_prompt_encoder = PromptEncoder(\n                type=args.encoder_prompt_type,\n                length=args.encoder_prompt_length,\n                projection=args.encoder_prompt_projection,\n                embed_dim=args.encoder_embed_dim,\n                proj_dim=args.encoder_prompt_dim,\n                layers=args.encoder_layers,\n                vocab_size=args.vocab_size)\n        self.encoder_dropout = nn.Dropout(p=0.2)\n        \n        self.dropout_module = FairseqDropout(\n            args.dropout, module_name=self.__class__.__name__\n        )\n        self.encoder_layerdrop = args.encoder_layerdrop\n\n        embed_dim = embed_tokens.embedding_dim\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = args.max_source_positions\n        self.num_attention_heads = args.encoder_attention_heads\n\n        self.embed_tokens = embed_tokens\n\n        self.embed_scale = 1.0 if args.no_scale_embedding else math.sqrt(embed_dim)\n\n        if getattr(args, \"layernorm_embedding\", False):\n            self.layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.layernorm_embedding = None\n\n        if getattr(args, \"add_type_embedding\", False):\n            self.type_embedding = Embedding(2, embed_dim, padding_idx=None)\n        else:\n            self.type_embedding = None\n\n        if getattr(args, \"sync_bn\", False):\n            norm_layer = BatchNorm2d\n        else:\n            if getattr(args, \"freeze_resnet\", False):\n                norm_layer = FrozenBatchNorm2d\n            else:\n                norm_layer = None\n\n        if args.resnet_type == 'resnet101':\n            self.embed_images = ResNet([3, 4, 23], norm_layer=norm_layer, drop_path_rate=args.resnet_drop_path_rate)\n        elif args.resnet_type == 'resnet152':\n            self.embed_images = ResNet([3, 8, 36], norm_layer=norm_layer, drop_path_rate=args.resnet_drop_path_rate)\n        elif args.resnet_type == 'resnet50':\n            self.embed_images = ResNet([3, 4, 6], norm_layer=norm_layer, drop_path_rate=args.resnet_drop_path_rate)\n        else:\n            raise NotImplementedError\n        self.image_proj = Linear(1024, embed_dim)\n        if getattr(args, \"resnet_model_path\", None):\n            print(\"load resnet {}\".format(args.resnet_model_path))\n            resnet_state_dict = torch.load(self.args.resnet_model_path)\n            self.embed_images.load_state_dict(resnet_state_dict)\n        if getattr(args, \"patch_layernorm_embedding\", False):\n            self.patch_layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.patch_layernorm_embedding = None\n\n        self.embed_positions = Embedding(args.max_source_positions + 2, embed_dim)\n        self.embed_image_positions = Embedding(args.image_bucket_size ** 2 + 1, embed_dim)\n        self.pos_ln = LayerNorm(embed_dim)\n        self.image_pos_ln = LayerNorm(embed_dim)\n        self.pos_scaling = float(embed_dim / args.encoder_attention_heads * args.attn_scale_factor) ** -0.5\n        self.pos_q_linear = nn.Linear(embed_dim, embed_dim)\n        self.pos_k_linear = nn.Linear(embed_dim, embed_dim)\n\n        if not args.adaptive_input and args.quant_noise_pq > 0:\n            self.quant_noise = apply_quant_noise_(\n                nn.Linear(embed_dim, embed_dim, bias=False),\n                args.quant_noise_pq,\n                args.quant_noise_pq_block_size,\n            )\n        else:\n            self.quant_noise = None\n\n        if self.encoder_layerdrop > 0.0:\n            self.layers = LayerDropModuleList(p=self.encoder_layerdrop)\n        else:\n            self.layers = nn.ModuleList([])\n\n        dpr = [x.item() for x in torch.linspace(0, args.encoder_drop_path_rate, args.encoder_layers)]\n        self.layers.extend(\n            [self.build_encoder_layer(args, drop_path_rate=dpr[i]) for i in range(args.encoder_layers)]\n        )\n        self.num_layers = len(self.layers)\n\n        if args.encoder_normalize_before:\n            self.layer_norm = LayerNorm(embed_dim)\n        else:\n            self.layer_norm = None\n\n        token_bucket_size = args.token_bucket_size\n        token_num_rel_dis = 2 * token_bucket_size - 1\n        token_rp_bucket = make_token_bucket_position(token_bucket_size)\n        self.token_rel_pos_table_list = nn.ModuleList(\n            [Embedding(token_num_rel_dis, self.num_attention_heads, zero_init=True) for _ in range(args.encoder_layers)]\n        )\n\n        image_bucket_size = args.image_bucket_size\n        image_num_rel_dis = (2 * image_bucket_size - 1) * (2 * image_bucket_size - 1) + 3\n        image_rp_bucket = make_image_bucket_position(image_bucket_size, image_num_rel_dis)\n        self.image_rel_pos_table_list = nn.ModuleList(\n            [Embedding(image_num_rel_dis, self.num_attention_heads, zero_init=True) for _ in range(args.encoder_layers)]\n        )\n\n        self.patch_image_size = args.patch_image_size\n        self.orig_patch_image_size = args.orig_patch_image_size\n\n        self.register_buffer(\"token_rp_bucket\", token_rp_bucket)\n        self.register_buffer(\"image_rp_bucket\", image_rp_bucket)\n        self.entangle_position_embedding = args.entangle_position_embedding\n\n    def build_encoder_layer(self, args, drop_path_rate=0.0):\n        layer = TransformerEncoderLayer(args, drop_path_rate=drop_path_rate, \\\n            use_adapter=getattr(args, \"adapter\", False), adapter_dim=getattr(args, \"adapter_dim\", 200))\n        checkpoint = getattr(args, \"checkpoint_activations\", False)\n        if checkpoint:\n            offload_to_cpu = getattr(args, \"offload_activations\", False)\n            layer = checkpoint_wrapper(layer, offload_to_cpu=offload_to_cpu)\n        # if we are checkpointing, enforce that FSDP always wraps the\n        # checkpointed layer, regardless of layer size\n        min_params_to_wrap = (\n            getattr(args, \"min_params_to_wrap\", DEFAULT_MIN_PARAMS_TO_WRAP)\n            if not checkpoint else 0\n        )\n        layer = fsdp_wrap(layer, min_num_params=min_params_to_wrap)\n        return layer\n\n    def get_rel_pos_bias(self, x, idx):\n        seq_len = x.size(1)\n        rp_bucket = self.token_rp_bucket[:seq_len, :seq_len]\n        values = F.embedding(rp_bucket, self.token_rel_pos_table_list[idx].weight)\n        values = values.unsqueeze(0).expand(x.size(0), -1, -1, -1)\n        values = values.permute([0, 3, 1, 2])\n        return values.contiguous()\n\n    def get_image_rel_pos_bias(self, image_position_ids, idx):\n        bsz, seq_len = image_position_ids.shape\n        rp_bucket_size = self.image_rp_bucket.size(1)\n\n        rp_bucket = self.image_rp_bucket.unsqueeze(0).expand(\n            bsz, rp_bucket_size, rp_bucket_size\n        ).gather(1, image_position_ids[:, :, None].expand(bsz, seq_len, rp_bucket_size)\n        ).gather(2, image_position_ids[:, None, :].expand(bsz, seq_len, seq_len))\n        values = F.embedding(rp_bucket, self.image_rel_pos_table_list[idx].weight)\n        values = values.permute(0, 3, 1, 2)\n        return values\n\n    def get_patch_images_info(self, patch_images, sample_patch_num, device):\n        image_embed = self.embed_images(patch_images)\n        h, w = image_embed.shape[-2:]\n        image_num_patches = h * w\n        image_padding_mask = patch_images.new_zeros((patch_images.size(0), image_num_patches)).bool()\n        image_position_idx = torch.arange(w).unsqueeze(0).expand(h, w) + \\\n                             torch.arange(h).unsqueeze(1) * self.args.image_bucket_size + 1\n        image_position_idx = image_position_idx.view(-1).to(device)\n        image_position_ids = image_position_idx[None, :].expand(patch_images.size(0), image_num_patches)\n\n        image_embed = image_embed.flatten(2).transpose(1, 2)\n        if sample_patch_num is not None:\n            patch_orders = [\n                random.sample(range(image_num_patches), k=sample_patch_num)\n                for _ in range(patch_images.size(0))\n            ]\n            patch_orders = torch.LongTensor(patch_orders).to(device)\n            image_embed = image_embed.gather(\n                1, patch_orders.unsqueeze(2).expand(-1, -1, image_embed.size(2))\n            )\n            image_num_patches = sample_patch_num\n            image_padding_mask = image_padding_mask.gather(1, patch_orders)\n            image_position_ids = image_position_ids.gather(1, patch_orders)\n        orig_num_patches = (self.orig_patch_image_size // 16) ** 2\n        orig_hw= self.orig_patch_image_size // 16\n        if getattr(self.args, \"interpolate_position\", False) and image_num_patches > orig_num_patches:\n            old_image_position_ids = torch.arange(orig_hw).unsqueeze(0).expand(orig_hw, orig_hw) + \\\n                                     torch.arange(orig_hw).unsqueeze(1) * self.args.image_bucket_size + 1\n            old_image_position_ids = old_image_position_ids.to(device)\n            old_image_pos_embed = self.embed_image_positions(old_image_position_ids)\n            old_image_pos_embed = old_image_pos_embed.reshape(1, orig_hw, orig_hw, -1).permute(0, 3, 1, 2)\n            image_pos_embed = F.interpolate(old_image_pos_embed, size=(h, w), mode='bilinear')\n            image_pos_embed = image_pos_embed.permute(0, 2, 3, 1).reshape(1, image_num_patches, -1)\n            image_pos_embed = image_pos_embed.expand(patch_images.size(0), -1, -1)\n        else:\n            image_pos_embed = self.embed_image_positions(image_position_ids)\n\n        return image_embed, image_num_patches, image_padding_mask, image_position_ids, image_pos_embed\n\n    def get_encoder_prompt(self, prompt_tokens):\n        past_key_values = self.encoder_prompt_encoder(prompt_tokens)\n        bsz, seqlen, _ = past_key_values.shape\n        past_key_values = past_key_values.view(\n            bsz,\n            seqlen,\n            (self.args.encoder_layers) * 2,\n            self.args.encoder_attention_heads,\n            self.args.encoder_embed_dim // self.args.encoder_attention_heads,\n        )\n        past_key_values = self.encoder_dropout(past_key_values)\n        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n        return past_key_values\n    \n    def forward_embedding(\n        self,\n        src_tokens,\n        image_embed: Optional[torch.Tensor] = None,\n        image_embed_2: Optional[torch.Tensor] = None,\n        token_embedding: Optional[torch.Tensor] = None,\n        pos_embed: Optional[torch.Tensor] = None,\n        image_pos_embed: Optional[torch.Tensor] = None,\n        image_pos_embed_2: Optional[torch.Tensor] = None\n    ):\n        # embed tokens and positions\n        if token_embedding is None:\n            token_embedding = self.embed_tokens(src_tokens)\n        x = embed = self.embed_scale * token_embedding\n        if self.entangle_position_embedding and pos_embed is not None:\n            x += pos_embed\n        if self.type_embedding is not None:\n            x += self.type_embedding(src_tokens.new_zeros(x.size()[:2]))\n        if self.layernorm_embedding is not None:\n            x = self.layernorm_embedding(x)\n        x = self.dropout_module(x)\n        if self.quant_noise is not None:\n            x = self.quant_noise(x)\n\n        # embed raw images\n        if image_embed is not None:\n            image_embed = self.image_proj(image_embed)\n            image_x = image_embed = self.embed_scale * image_embed\n            if self.entangle_position_embedding and image_pos_embed is not None:\n                image_x += image_pos_embed\n            if self.type_embedding is not None:\n                image_x += self.type_embedding(src_tokens.new_ones(image_x.size()[:2]))\n            if self.patch_layernorm_embedding is not None:\n                image_x = self.patch_layernorm_embedding(image_x)\n            image_x = self.dropout_module(image_x)\n            if self.quant_noise is not None:\n                image_x = self.quant_noise(image_x)\n            x = torch.cat([image_x, x], dim=1)\n            embed = torch.cat([image_embed, embed], dim=1)\n\n        if image_embed_2 is not None:\n            assert self.type_embedding is not None\n            image_embed_2 = self.image_proj(image_embed_2)\n            image_x_2 = image_embed_2 = self.embed_scale * image_embed_2\n            if self.entangle_position_embedding and image_pos_embed_2 is not None:\n                image_x_2 += image_pos_embed_2\n            if self.type_embedding is not None:\n                image_x_2 += self.type_embedding(src_tokens.new_full(image_x_2.size()[:2], fill_value=2))\n            if self.patch_layernorm_embedding is not None:\n                image_x_2 = self.patch_layernorm_embedding(image_x_2)\n            image_x_2 = self.dropout_module(image_x_2)\n            if self.quant_noise is not None:\n                image_x_2 = self.quant_noise(image_x_2)\n            x = torch.cat([image_x_2, x], dim=1)\n            embed = torch.cat([image_embed_2, embed], dim=1)\n\n        return x, embed\n\n    def forward(\n        self,\n        src_tokens,\n        src_lengths,\n        patch_images: Optional[torch.Tensor] = None,\n        patch_images_2: Optional[torch.Tensor] = None,\n        patch_masks: Optional[torch.Tensor] = None,\n        code_masks: Optional[torch.Tensor] = None,\n        return_all_hiddens: bool = False,\n        token_embeddings: Optional[torch.Tensor] = None,\n        sample_patch_num: Optional[int] = None\n    ):\n        \"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (torch.LongTensor): lengths of each source sentence of\n                shape `(batch)`\n            return_all_hiddens (bool, optional): also return all of the\n                intermediate hidden states (default: False).\n            token_embeddings (torch.Tensor, optional): precomputed embeddings\n                default `None` will recompute embeddings\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer's output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n                - **encoder_embedding** (Tensor): the (scaled) embedding lookup\n                  of shape `(batch, src_len, embed_dim)`\n                - **encoder_states** (List[Tensor]): all intermediate\n                  hidden states of shape `(src_len, batch, embed_dim)`.\n                  Only populated if *return_all_hiddens* is True.\n        \"\"\"\n        return self.forward_scriptable(src_tokens,\n                                       src_lengths,\n                                       patch_images,\n                                       patch_images_2,\n                                       patch_masks,\n                                       return_all_hiddens,\n                                       token_embeddings,\n                                       sample_patch_num)\n\n    # TorchScript doesn't support super() method so that the scriptable Subclass\n    # can't access the base class model in Torchscript.\n    # Current workaround is to add a helper function with different name and\n    # call the helper function from scriptable Subclass.\n    def forward_scriptable(\n        self,\n        src_tokens,\n        src_lengths,\n        patch_images: Optional[torch.Tensor] = None,\n        patch_images_2: Optional[torch.Tensor] = None,\n        patch_masks: Optional[torch.Tensor] = None,\n        return_all_hiddens: bool = False,\n        token_embeddings: Optional[torch.Tensor] = None,\n        sample_patch_num: Optional[int] = None\n    ):\n        \"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (torch.LongTensor): lengths of each source sentence of\n                shape `(batch)`\n            return_all_hiddens (bool, optional): also return all of the\n                intermediate hidden states (default: False).\n            token_embeddings (torch.Tensor, optional): precomputed embeddings\n                default `None` will recompute embeddings\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer's output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n                - **encoder_embedding** (Tensor): the (scaled) embedding lookup\n                  of shape `(batch, src_len, embed_dim)`\n                - **encoder_states** (List[Tensor]): all intermediate\n                  hidden states of shape `(src_len, batch, embed_dim)`.\n                  Only populated if *return_all_hiddens* is True.\n        \"\"\"\n        prompt_tokens = None\n        prompt_padding_mask = None\n        prompt_kv_list = None\n        if self.args.encoder_prompt:\n            bsz, seq_len = src_tokens.shape[0], src_tokens.shape[1]\n            if self.args.encoder_prompt_type in (\"prefix\"):\n                prompt_tokens = torch.arange(\n                    0, self.args.encoder_prompt_length).to(\n                    src_tokens.device)\n                prompt_tokens = prompt_tokens.unsqueeze(0).expand(bsz, -1)\n                prompt_padding_mask = torch.zeros_like(prompt_tokens).to(prompt_tokens.device)\n            prompt_kv_list = self.get_encoder_prompt(prompt_tokens)\n        image_embed = None\n        image_embed_2 = None\n        image_pos_embed = None\n        image_pos_embed_2 = None\n        if patch_images is not None:\n            image_embed, image_num_patches, image_padding_mask, image_position_ids, image_pos_embed = \\\n                self.get_patch_images_info(patch_images, sample_patch_num, src_tokens.device)\n            image_padding_mask[~patch_masks] = True\n        if patch_images_2 is not None:\n            image_embed_2, image_num_patches_2, image_padding_mask_2, image_position_ids_2, image_pos_embed_2 = \\\n                self.get_patch_images_info(patch_images_2, sample_patch_num, src_tokens.device)\n            image_padding_mask_2[~patch_masks] = True\n\n        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n        if patch_images is not None:\n            encoder_padding_mask = torch.cat([image_padding_mask, encoder_padding_mask], dim=1)\n        if patch_images_2 is not None:\n            encoder_padding_mask = torch.cat([image_padding_mask_2, encoder_padding_mask], dim=1)\n        has_pads = (src_tokens.device.type == \"xla\" or encoder_padding_mask.any())\n\n        pos_embed = self.embed_positions(utils.new_arange(src_tokens))\n        x, encoder_embedding = self.forward_embedding(\n            src_tokens, image_embed, image_embed_2, token_embeddings,\n            pos_embed, image_pos_embed, image_pos_embed_2\n        )\n\n        # account for padding while computing the representation\n        if has_pads:\n            x = x * (1 - encoder_padding_mask.unsqueeze(-1).type_as(x))\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        pos_embed = self.pos_ln(pos_embed)\n        if patch_images is not None:\n            image_pos_embed = self.image_pos_ln(image_pos_embed)\n            pos_embed = torch.cat([image_pos_embed, pos_embed], dim=1)\n        if patch_images_2 is not None:\n            image_pos_embed_2 = self.image_pos_ln(image_pos_embed_2)\n            pos_embed = torch.cat([image_pos_embed_2, pos_embed], dim=1)\n\n        pos_q = self.pos_q_linear(pos_embed).view(\n            pos_embed.size(0), pos_embed.size(1), self.num_attention_heads, -1\n        ).transpose(1, 2) * self.pos_scaling\n        pos_k = self.pos_k_linear(pos_embed).view(\n            pos_embed.size(0), pos_embed.size(1), self.num_attention_heads, -1\n        ).transpose(1, 2)\n        abs_pos_bias = torch.matmul(pos_q, pos_k.transpose(2, 3))\n\n        encoder_states = []\n\n        if return_all_hiddens:\n            encoder_states.append(x)\n\n        if prompt_padding_mask is not None:\n            encoder_padding_mask = torch.cat([prompt_padding_mask, encoder_padding_mask], dim=1)\n        # encoder layers\n        for idx, layer in enumerate(self.layers):\n            self_attn_bias = abs_pos_bias.clone()\n            self_attn_bias[:, :, -src_tokens.size(1):, -src_tokens.size(1):] += self.get_rel_pos_bias(src_tokens, idx)\n            if patch_images_2 is not None:\n                self_attn_bias[:, :, :image_num_patches_2, :image_num_patches_2] += \\\n                    self.get_image_rel_pos_bias(image_position_ids_2, idx)\n                self_attn_bias[:, :, image_num_patches_2:image_num_patches_2+image_num_patches, image_num_patches_2:image_num_patches_2+image_num_patches] += \\\n                    self.get_image_rel_pos_bias(image_position_ids, idx)\n            elif patch_images is not None:\n                self_attn_bias[:, :, :x.size(0) - src_tokens.size(1), :x.size(0) - src_tokens.size(1)] += \\\n                    self.get_image_rel_pos_bias(image_position_ids, idx)\n            self_attn_bias = self_attn_bias.reshape(-1, self_attn_bias.size(2), self_attn_bias.size(2))\n            if self.args.encoder_prompt:\n                if self.args.encoder_prompt_type != \"prompt\":\n                    prompt_kv = prompt_kv_list[idx]\n                else:\n                    if idx == 0:\n                        prompt_kv = prompt_kv_list[idx]\n                    else:\n                        prompt_kv = None\n            else:\n                prompt_kv = None \n            x = layer(x, encoder_padding_mask=encoder_padding_mask if has_pads else None, \\\n                    self_attn_bias=self_attn_bias, prompt_kv=prompt_kv)\n            if return_all_hiddens:\n                assert encoder_states is not None\n                encoder_states.append(x)\n\n        if self.layer_norm is not None:\n            x = self.layer_norm(x)\n        if self.args.encoder_prompt:\n            encoder_padding_mask = encoder_padding_mask[:, prompt_tokens.size(1):]\n        # The Pytorch Mobile lite interpreter does not supports returning NamedTuple in\n        # `forward` so we use a dictionary instead.\n        # TorchScript does not support mixed values so the values are all lists.\n        # The empty list is equivalent to None.\n        return {\n            \"encoder_out\": [x],  # T x B x C\n            \"encoder_padding_mask\": [encoder_padding_mask],  # B x T\n            \"encoder_embedding\": [],  # B x T x C\n            \"encoder_states\": encoder_states,  # List[T x B x C]\n            \"src_tokens\": [],\n            \"src_lengths\": [],\n            \"position_embeddings\": [pos_embed],  # B x T x C\n        }\n\n    @torch.jit.export\n    def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):\n        \"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"\n        if len(encoder_out[\"encoder_out\"]) == 0:\n            new_encoder_out = []\n        else:\n            new_encoder_out = [encoder_out[\"encoder_out\"][0].index_select(1, new_order)]\n        if len(encoder_out[\"encoder_padding_mask\"]) == 0:\n            new_encoder_padding_mask = []\n        else:\n            new_encoder_padding_mask = [\n                encoder_out[\"encoder_padding_mask\"][0].index_select(0, new_order)\n            ]\n        if len(encoder_out[\"encoder_embedding\"]) == 0:\n            new_encoder_embedding = []\n        else:\n            new_encoder_embedding = [\n                encoder_out[\"encoder_embedding\"][0].index_select(0, new_order)\n            ]\n\n        if len(encoder_out[\"src_tokens\"]) == 0:\n            new_src_tokens = []\n        else:\n            new_src_tokens = [(encoder_out[\"src_tokens\"][0]).index_select(0, new_order)]\n\n        if len(encoder_out[\"src_lengths\"]) == 0:\n            new_src_lengths = []\n        else:\n            new_src_lengths = [(encoder_out[\"src_lengths\"][0]).index_select(0, new_order)]\n\n        if len(encoder_out[\"position_embeddings\"]) == 0:\n            new_position_embeddings = []\n        else:\n            new_position_embeddings = [(encoder_out[\"position_embeddings\"][0]).index_select(0, new_order)]\n\n        encoder_states = encoder_out[\"encoder_states\"]\n        if len(encoder_states) > 0:\n            for idx, state in enumerate(encoder_states):\n                encoder_states[idx] = state.index_select(1, new_order)\n\n        return {\n            \"encoder_out\": new_encoder_out,  # T x B x C\n            \"encoder_padding_mask\": new_encoder_padding_mask,  # B x T\n            \"encoder_embedding\": new_encoder_embedding,  # B x T x C\n            \"encoder_states\": encoder_states,  # List[T x B x C]\n            \"src_tokens\": new_src_tokens,  # B x T\n            \"src_lengths\": new_src_lengths,  # B x 1\n            \"position_embeddings\": new_position_embeddings,  # B x T x C\n        }\n\n    def max_positions(self):\n        \"\"\"Maximum input length supported by the encoder.\"\"\"\n        if self.embed_positions is None:\n            return self.max_source_positions\n        return self.max_source_positions\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        \"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n            weights_key = \"{}.embed_positions.weights\".format(name)\n            if weights_key in state_dict:\n                print(\"deleting {0}\".format(weights_key))\n                del state_dict[weights_key]\n            state_dict[\n                \"{}.embed_positions._float_tensor\".format(name)\n            ] = torch.FloatTensor(1)\n        for i in range(self.num_layers):\n            # update layer norms\n            self.layers[i].upgrade_state_dict_named(\n                state_dict, \"{}.layers.{}\".format(name, i)\n            )\n\n        # version_key = \"{}.version\".format(name)\n        # if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) < 2:\n        #     # earlier checkpoints did not normalize after the stack of layers\n        #     self.layer_norm = None\n        #     self.normalize = False\n        #     state_dict[version_key] = torch.Tensor([1])\n\n        prefix = name + \".\" if name != \"\" else \"\"\n        for param_name, param_tensor in self.state_dict().items():\n            if (prefix + param_name) not in state_dict:\n                state_dict[prefix + param_name] = self.state_dict()[param_name]\n\n        if len(state_dict[\"encoder.embed_image_positions.weight\"]) < len(self.state_dict()[\"embed_image_positions.weight\"]):\n            num_posids_to_add = len(self.state_dict()[\"embed_image_positions.weight\"]) - len(state_dict[\"encoder.embed_image_positions.weight\"])\n            embed_dim = state_dict[\"encoder.embed_image_positions.weight\"].size(1)\n            new_pos_embed_to_add = torch.zeros(num_posids_to_add, embed_dim)\n            nn.init.normal_(new_pos_embed_to_add, mean=0, std=embed_dim ** -0.5)\n            new_pos_embed_to_add = new_pos_embed_to_add.to(\n                dtype=state_dict[\"encoder.embed_image_positions.weight\"].dtype,\n            )\n            state_dict[\"encoder.embed_image_positions.weight\"] = torch.cat(\n                [state_dict[\"encoder.embed_image_positions.weight\"], new_pos_embed_to_add]\n            )\n        return state_dict\n\n\nclass TransformerDecoder(FairseqIncrementalDecoder):\n    \"\"\"\n    Transformer decoder consisting of *args.decoder_layers* layers. Each layer\n    is a :class:`TransformerDecoderLayer`.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): decoding dictionary\n        embed_tokens (torch.nn.Embedding): output embedding\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n            (default: False).\n    \"\"\"\n\n    def __init__(\n        self,\n        args,\n        dictionary,\n        embed_tokens,\n        no_encoder_attn=False,\n        output_projection=None,\n    ):\n        self.args = args\n        super().__init__(dictionary)\n        self.register_buffer(\"version\", torch.Tensor([3]))\n        self._future_mask = torch.empty(0)\n\n        if getattr(args, \"decoder_prompt\", False):\n            self.decoder_prompt_encoder = PromptEncoder(\n                type=args.decoder_prompt_type,\n                length=args.decoder_prompt_length,\n                projection=args.decoder_prompt_projection,\n                embed_dim=args.decoder_embed_dim,\n                proj_dim=args.decoder_prompt_dim,\n                layers=args.decoder_layers,\n                vocab_size=args.vocab_size)\n            self.decoder_dropout = nn.Dropout(p=0.2)\n\n        self.dropout_module = FairseqDropout(\n            args.dropout, module_name=self.__class__.__name__\n        )\n        self.decoder_layerdrop = args.decoder_layerdrop\n        self.share_input_output_embed = args.share_decoder_input_output_embed\n        self.num_attention_heads = args.decoder_attention_heads\n\n        input_embed_dim = embed_tokens.embedding_dim\n        embed_dim = args.decoder_embed_dim\n        self.embed_dim = embed_dim\n        self.output_embed_dim = args.decoder_output_dim\n\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_target_positions = args.max_target_positions\n\n        self.embed_tokens = embed_tokens\n\n        self.embed_scale = 1.0 if args.no_scale_embedding else math.sqrt(embed_dim)\n\n        if not args.adaptive_input and args.quant_noise_pq > 0:\n            self.quant_noise = apply_quant_noise_(\n                nn.Linear(embed_dim, embed_dim, bias=False),\n                args.quant_noise_pq,\n                args.quant_noise_pq_block_size,\n            )\n        else:\n            self.quant_noise = None\n\n        self.project_in_dim = (\n            Linear(input_embed_dim, embed_dim, bias=False)\n            if embed_dim != input_embed_dim\n            else None\n        )\n\n        if getattr(args, \"layernorm_embedding\", False):\n            self.layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.layernorm_embedding = None\n\n        self.window_size = args.code_image_size // 8\n\n        self.embed_positions = Embedding(args.max_target_positions + 2, embed_dim)\n        self.embed_image_positions = Embedding(args.image_bucket_size ** 2 + 1, embed_dim)\n        self.pos_ln = LayerNorm(embed_dim)\n        self.image_pos_ln = LayerNorm(embed_dim)\n        self.pos_scaling = float(embed_dim / self.num_attention_heads * args.attn_scale_factor) ** -0.5\n        self.self_pos_q_linear = nn.Linear(embed_dim, embed_dim)\n        self.self_pos_k_linear = nn.Linear(embed_dim, embed_dim)\n        self.cross_pos_q_linear = nn.Linear(embed_dim, embed_dim)\n        self.cross_pos_k_linear = nn.Linear(embed_dim, embed_dim)\n\n        if getattr(args, \"code_layernorm_embedding\", False):\n            self.code_layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.code_layernorm_embedding = None\n\n        self.cross_self_attention = getattr(args, \"cross_self_attention\", False)\n\n        if self.decoder_layerdrop > 0.0:\n            self.layers = LayerDropModuleList(p=self.decoder_layerdrop)\n        else:\n            self.layers = nn.ModuleList([])\n\n        dpr = [x.item() for x in torch.linspace(0, args.decoder_drop_path_rate, args.decoder_layers)]\n        self.layers.extend(\n            [\n                self.build_decoder_layer(args, no_encoder_attn, drop_path_rate=dpr[i])\n                for i in range(args.decoder_layers)\n            ]\n        )\n        self.num_layers = len(self.layers)\n\n        if args.decoder_normalize_before:\n            self.layer_norm = LayerNorm(embed_dim)\n        else:\n            self.layer_norm = None\n\n        self.project_out_dim = (\n            Linear(embed_dim, self.output_embed_dim, bias=False)\n            if embed_dim != self.output_embed_dim and not args.tie_adaptive_weights\n            else None\n        )\n\n        self.adaptive_softmax = None\n        self.output_projection = output_projection\n        if self.output_projection is None:\n            self.build_output_projection(args, dictionary, embed_tokens)\n\n        token_bucket_size = args.token_bucket_size\n        token_num_rel_dis = 2 * token_bucket_size - 1\n        token_rp_bucket = make_token_bucket_position(token_bucket_size)\n        self.token_rel_pos_table_list = nn.ModuleList(\n            [Embedding(token_num_rel_dis, self.num_attention_heads, zero_init=True) for _ in range(args.decoder_layers)]\n        )\n\n        image_bucket_size = args.image_bucket_size\n        image_num_rel_dis = (2 * image_bucket_size - 1) * (2 * image_bucket_size - 1) + 3\n        image_rp_bucket = make_image_bucket_position(image_bucket_size, image_num_rel_dis)\n        image_position_idx = torch.arange(self.window_size).unsqueeze(0).expand(self.window_size, self.window_size) + \\\n                             torch.arange(self.window_size).unsqueeze(1) * image_bucket_size + 1\n        image_position_idx = torch.cat([torch.tensor([0]), image_position_idx.view(-1)])\n        image_position_idx = torch.cat([image_position_idx, torch.tensor([1024] * 769)])\n        self.image_rel_pos_table_list = nn.ModuleList(\n            [Embedding(image_num_rel_dis, self.num_attention_heads, zero_init=True) for _ in range(args.decoder_layers)]\n        )\n\n        self.register_buffer(\"token_rp_bucket\", token_rp_bucket)\n        self.register_buffer(\"image_rp_bucket\", image_rp_bucket)\n        self.register_buffer(\"image_position_idx\", image_position_idx)\n        self.entangle_position_embedding = args.entangle_position_embedding\n\n    def get_decoder_prompt(self, prompt_tokens):\n        past_key_values = self.decoder_prompt_encoder(prompt_tokens)\n        bsz, seqlen, _ = past_key_values.shape\n        past_key_values = past_key_values.view(\n            bsz,\n            seqlen,\n            self.args.decoder_layers * 2,\n            self.args.decoder_attention_heads,\n            self.args.decoder_embed_dim // self.args.decoder_attention_heads,\n        )\n        past_key_values = self.decoder_dropout(past_key_values)\n        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n        return past_key_values\n\n    def build_output_projection(self, args, dictionary, embed_tokens):\n        if args.adaptive_softmax_cutoff is not None:\n            self.adaptive_softmax = AdaptiveSoftmax(\n                len(dictionary),\n                self.output_embed_dim,\n                utils.eval_str_list(args.adaptive_softmax_cutoff, type=int),\n                dropout=args.adaptive_softmax_dropout,\n                adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None,\n                factor=args.adaptive_softmax_factor,\n                tie_proj=args.tie_adaptive_proj,\n            )\n        elif self.share_input_output_embed:\n            self.output_projection = nn.Linear(\n                self.embed_tokens.weight.shape[1],\n                self.embed_tokens.weight.shape[0],\n                bias=False,\n            )\n            self.output_projection.weight = self.embed_tokens.weight\n        else:\n            self.output_projection = nn.Linear(\n                self.output_embed_dim, len(dictionary), bias=False\n            )\n            nn.init.normal_(\n                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n            )\n        num_base_layers = getattr(args, \"base_layers\", 0)\n        for i in range(num_base_layers):\n            self.layers.insert(((i+1) * args.decoder_layers) // (num_base_layers + 1), BaseLayer(args))\n\n    def build_decoder_layer(self, args, no_encoder_attn=False, drop_path_rate=0.0):\n        layer = TransformerDecoderLayer(args, no_encoder_attn, drop_path_rate= \\\n            drop_path_rate, use_adapter=getattr(args, \"adapter\", False), adapter_dim=getattr(args, \"adapter_dim\", 200))\n        checkpoint = getattr(args, \"checkpoint_activations\", False)\n        if checkpoint:\n            offload_to_cpu = getattr(args, \"offload_activations\", False)\n            layer = checkpoint_wrapper(layer, offload_to_cpu=offload_to_cpu)\n        # if we are checkpointing, enforce that FSDP always wraps the\n        # checkpointed layer, regardless of layer size\n        min_params_to_wrap = (\n            getattr(args, \"min_params_to_wrap\", DEFAULT_MIN_PARAMS_TO_WRAP)\n            if not checkpoint else 0\n        )\n        layer = fsdp_wrap(layer, min_num_params=min_params_to_wrap)\n        return layer\n\n    def get_rel_pos_bias(self, x, idx):\n        seq_len = x.size(1)\n        rp_bucket = self.token_rp_bucket[:seq_len, :seq_len]\n        values = F.embedding(rp_bucket, self.token_rel_pos_table_list[idx].weight)\n        values = values.permute([2, 0, 1])\n        return values.contiguous()\n\n    def get_image_rel_pos_bias(self, x, idx):\n        seq_len = x.size(1)\n        image_position_idx = self.image_position_idx[:seq_len]\n        rp_bucket = self.image_rp_bucket[image_position_idx][:, image_position_idx]\n        values = F.embedding(rp_bucket, self.image_rel_pos_table_list[idx].weight)\n        values = values.permute(2, 0, 1)\n        return values\n\n    def get_pos_info(self, tokens, tgt_pos_embed, src_pos_embed=None, use_image=False):\n        batch_size = tokens.size(0)\n        tgt_len = tokens.size(1)\n        tgt_pos_embed = self.image_pos_ln(tgt_pos_embed) if use_image else self.pos_ln(tgt_pos_embed)\n        if src_pos_embed is not None:\n            src_len = src_pos_embed.size(1)\n            pos_q = self.cross_pos_q_linear(tgt_pos_embed).view(\n                batch_size, tgt_len, self.num_attention_heads, -1\n            ).transpose(1, 2) * self.pos_scaling\n            pos_k = self.cross_pos_k_linear(src_pos_embed).view(\n                batch_size, src_len, self.num_attention_heads, -1\n            ).transpose(1, 2)\n        else:\n            src_len = tgt_pos_embed.size(1)\n            pos_q = self.self_pos_q_linear(tgt_pos_embed).view(\n                batch_size, tgt_len, self.num_attention_heads, -1\n            ).transpose(1, 2) * self.pos_scaling\n            pos_k = self.self_pos_k_linear(tgt_pos_embed).view(\n                batch_size, src_len, self.num_attention_heads, -1\n            ).transpose(1, 2)\n        abs_pos_bias = torch.matmul(pos_q, pos_k.transpose(2, 3))\n        return abs_pos_bias\n\n    def forward(\n        self,\n        prev_output_tokens,\n        code_masks: Optional[torch.Tensor] = None,\n        encoder_out: Optional[Dict[str, List[Tensor]]] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        features_only: bool = False,\n        full_context_alignment: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n        src_lengths: Optional[Any] = None,\n        return_all_hiddens: bool = False,\n    ):\n        \"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (optional): output from the encoder, used for\n                encoder-side attention, should be of size T x B x C\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n            features_only (bool, optional): only return features without\n                applying output layer (default: False).\n            full_context_alignment (bool, optional): don't apply\n                auto-regressive mask to self-attention (default: False).\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"\n\n        x, extra = self.extract_features(\n            prev_output_tokens,\n            code_masks=code_masks,\n            encoder_out=encoder_out,\n            incremental_state=incremental_state,\n            full_context_alignment=full_context_alignment,\n            alignment_layer=alignment_layer,\n            alignment_heads=alignment_heads,\n        )\n\n        if not features_only:\n            x = self.output_layer(x)\n        return x, extra\n\n    def extract_features(\n        self,\n        prev_output_tokens,\n        code_masks: Optional[torch.Tensor],\n        encoder_out: Optional[Dict[str, List[Tensor]]],\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        full_context_alignment: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n    ):\n        return self.extract_features_scriptable(\n            prev_output_tokens,\n            code_masks,\n            encoder_out,\n            incremental_state,\n            full_context_alignment,\n            alignment_layer,\n            alignment_heads,\n        )\n\n    \"\"\"\n    A scriptable subclass of this class has an extract_features method and calls\n    super().extract_features, but super() is not supported in torchscript. A copy of\n    this function is made to be used in the subclass instead.\n    \"\"\"\n\n    def extract_features_scriptable(\n        self,\n        prev_output_tokens,\n        code_masks: Optional[torch.Tensor],\n        encoder_out: Optional[Dict[str, List[Tensor]]],\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        full_context_alignment: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n    ):\n        \"\"\"\n        Similar to *forward* but only return features.\n\n        Includes several features from \"Jointly Learning to Align and\n        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n\n        Args:\n            full_context_alignment (bool, optional): don't apply\n                auto-regressive mask to self-attention (default: False).\n            alignment_layer (int, optional): return mean alignment over\n                heads at this layer (default: last layer).\n            alignment_heads (int, optional): only average alignment over\n                this many heads (default: all heads).\n\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"\n        prompt_tokens = None\n        prompt_padding_mask = None\n        prompt_kv_list = None\n        if self.args.decoder_prompt:\n            bsz, seq_len = prev_output_tokens.shape[0], prev_output_tokens.shape[1]\n            if self.args.decoder_prompt_type in (\"prefix\"):\n                prompt_tokens = torch.arange(\n                    0, self.args.decoder_prompt_length).to(\n                    prev_output_tokens.device)\n                prompt_tokens = prompt_tokens.unsqueeze(0).expand(bsz, -1)\n                prompt_padding_mask = torch.zeros_like(prompt_tokens).to(prompt_tokens.device)\n            prompt_kv_list = self.get_decoder_prompt(prompt_tokens)\n        bs, slen = prev_output_tokens.size()\n        if alignment_layer is None:\n            alignment_layer = self.num_layers - 1\n\n        enc: Optional[Tensor] = None\n        padding_mask: Optional[Tensor] = None\n        if encoder_out is not None and len(encoder_out[\"encoder_out\"]) > 0:\n            enc = encoder_out[\"encoder_out\"][0]\n            assert (\n                enc.size()[1] == bs\n            ), f\"Expected enc.shape == (t, {bs}, c) got {enc.shape}\"\n        if encoder_out is not None and len(encoder_out[\"encoder_padding_mask\"]) > 0:\n            padding_mask = encoder_out[\"encoder_padding_mask\"][0]\n\n        bsz, tgt_len = prev_output_tokens.shape\n        token_position_idx = utils.new_arange(prev_output_tokens)\n        tgt_pos_embed = self.embed_positions(token_position_idx)\n        if code_masks is not None and torch.any(code_masks):\n            image_position_idx = self.image_position_idx[:prev_output_tokens.size(1)].unsqueeze(0).expand(bsz, tgt_len)\n            tgt_pos_embed[code_masks] = self.embed_image_positions(image_position_idx)[code_masks]\n\n        # self attn position bias\n        self_abs_pos_bias = self.get_pos_info(prev_output_tokens, tgt_pos_embed, use_image=False)\n        if code_masks is not None and torch.any(code_masks):\n            self_image_abs_pos_bias = self.get_pos_info(prev_output_tokens, tgt_pos_embed, use_image=True)\n            self_abs_pos_bias[code_masks] = self_image_abs_pos_bias[code_masks]\n        # cross attn position bias\n        src_pos_embed = encoder_out['position_embeddings'][0]\n        cross_abs_pos_bias = self.get_pos_info(prev_output_tokens, tgt_pos_embed, src_pos_embed=src_pos_embed)\n        if code_masks is not None and torch.any(code_masks):\n            cross_image_abs_pos_bias = self.get_pos_info(prev_output_tokens, tgt_pos_embed, src_pos_embed=src_pos_embed, use_image=True)\n            cross_abs_pos_bias[code_masks] = cross_image_abs_pos_bias[code_masks]\n        cross_abs_pos_bias = cross_abs_pos_bias.reshape(-1, *cross_abs_pos_bias.size()[-2:])\n\n        all_prev_output_tokens = prev_output_tokens.clone()\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n            cross_abs_pos_bias = cross_abs_pos_bias[:, -1:, :]\n            tgt_pos_embed = tgt_pos_embed[:, -1:, :]\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n\n        if self.quant_noise is not None:\n            x = self.quant_noise(x)\n\n        if self.project_in_dim is not None:\n            x = self.project_in_dim(x)\n\n        if self.entangle_position_embedding is not None and not self.args.disable_entangle:\n            x += tgt_pos_embed\n\n        if self.layernorm_embedding is not None:\n            if code_masks is None or not code_masks.any() or not getattr(self, \"code_layernorm_embedding\", False):\n                x = self.layernorm_embedding(x)\n            elif code_masks is not None and code_masks.all():\n                x = self.code_layernorm_embedding(x)\n            else:\n                x[~code_masks] = self.layernorm_embedding(x[~code_masks])\n                x[code_masks] = self.code_layernorm_embedding(x[code_masks])\n\n        x = self.dropout_module(x)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        self_attn_padding_mask: Optional[Tensor] = None\n        if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():\n            self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)\n            if not incremental_state and prompt_padding_mask is not None:\n                self_attn_padding_mask = torch.cat([prompt_padding_mask, self_attn_padding_mask], dim=1)\n\n        # decoder layers\n        attn: Optional[Tensor] = None\n        inner_states: List[Optional[Tensor]] = [x]\n        for idx, layer in enumerate(self.layers):\n            if incremental_state is None and not full_context_alignment:\n                self_attn_mask = self.buffered_future_mask(x)\n                if self.args.decoder_prompt:\n                    seq_len, prompt_len = x.size(0), prompt_tokens.size(1)\n                    prompt_mask = torch.zeros([seq_len, prompt_len]).to(x.device)\n                    self_attn_mask = torch.cat([prompt_mask, self_attn_mask], dim=1)\n            else:\n                self_attn_mask = None\n\n            self_attn_bias = self_abs_pos_bias.clone()\n            if code_masks is None or not code_masks.any():\n                self_attn_bias += self.get_rel_pos_bias(all_prev_output_tokens, idx).unsqueeze(0)\n            elif code_masks is not None and code_masks.all():\n                self_attn_bias += self.get_image_rel_pos_bias(all_prev_output_tokens, idx).unsqueeze(0)\n            else:\n                self_attn_bias[~code_masks] += self.get_rel_pos_bias(all_prev_output_tokens, idx).unsqueeze(0)\n                self_attn_bias[code_masks] += self.get_image_rel_pos_bias(all_prev_output_tokens, idx).unsqueeze(0)\n            self_attn_bias = self_attn_bias.reshape(-1, *self_attn_bias.size()[-2:])\n            if incremental_state is not None:\n                self_attn_bias = self_attn_bias[:, -1:, :]\n\n            if self.args.decoder_prompt:\n                if self.args.decoder_prompt_type != \"prompt\":\n                    prompt_kv = prompt_kv_list[idx]\n                else:\n                    if idx == 0:\n                        prompt_kv = prompt_kv_list[idx]\n                    else:\n                        prompt_kv = None\n            else:\n                prompt_kv = None\n\n            x, layer_attn, _ = layer(\n                x,\n                enc,\n                padding_mask,\n                incremental_state,\n                self_attn_mask=self_attn_mask,\n                self_attn_padding_mask=self_attn_padding_mask,\n                need_attn=bool((idx == alignment_layer)),\n                need_head_weights=bool((idx == alignment_layer)),\n                self_attn_bias=self_attn_bias,\n                cross_attn_bias=cross_abs_pos_bias,\n                prompt_kv=prompt_kv\n            )\n            inner_states.append(x)\n            if layer_attn is not None and idx == alignment_layer:\n                attn = layer_attn.float().to(x)\n\n        if attn is not None:\n            if alignment_heads is not None:\n                attn = attn[:alignment_heads]\n\n            # average probabilities over heads\n            attn = attn.mean(dim=0)\n\n        if self.layer_norm is not None:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.project_out_dim is not None:\n            x = self.project_out_dim(x)\n\n        return x, {\"attn\": [attn], \"inner_states\": inner_states}\n\n    def output_layer(self, features):\n        \"\"\"Project features to the vocabulary size.\"\"\"\n        if self.adaptive_softmax is None:\n            # project back to size of vocabulary\n            return self.output_projection(features)\n        else:\n            return features\n\n    def max_positions(self):\n        \"\"\"Maximum output length supported by the decoder.\"\"\"\n        if self.embed_positions is None:\n            return self.max_target_positions\n        return self.max_target_positions\n\n    def buffered_future_mask(self, tensor):\n        dim = tensor.size(0)\n        # self._future_mask.device != tensor.device is not working in TorchScript. This is a workaround.\n        if (\n            self._future_mask.size(0) == 0\n            or (not self._future_mask.device == tensor.device)\n            or self._future_mask.size(0) < dim\n        ):\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(torch.zeros([dim, dim])), 1\n            )\n        self._future_mask = self._future_mask.to(tensor)\n        return self._future_mask[:dim, :dim]\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        \"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n            weights_key = \"{}.embed_positions.weights\".format(name)\n            if weights_key in state_dict:\n                del state_dict[weights_key]\n            state_dict[\n                \"{}.embed_positions._float_tensor\".format(name)\n            ] = torch.FloatTensor(1)\n\n        if f\"{name}.output_projection.weight\" not in state_dict:\n            if self.share_input_output_embed:\n                embed_out_key = f\"{name}.embed_tokens.weight\"\n            else:\n                embed_out_key = f\"{name}.embed_out\"\n            if embed_out_key in state_dict:\n                state_dict[f\"{name}.output_projection.weight\"] = state_dict[\n                    embed_out_key\n                ]\n                if not self.share_input_output_embed:\n                    del state_dict[embed_out_key]\n\n        for i in range(self.num_layers):\n            # update layer norms\n            self.layers[i].upgrade_state_dict_named(\n                state_dict, \"{}.layers.{}\".format(name, i)\n            )\n\n        # version_key = \"{}.version\".format(name)\n        # if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) <= 2:\n        #     # earlier checkpoints did not normalize after the stack of layers\n        #     self.layer_norm = None\n        #     self.normalize = False\n        #     state_dict[version_key] = torch.Tensor([1])\n\n        prefix = name + \".\" if name != \"\" else \"\"\n        image_params = [\"image_position_idx\"]\n        for image_param in image_params:\n            state_dict[prefix + image_param] = self.state_dict()[image_param]\n        for param_name, param_tensor in self.state_dict().items():\n            if (prefix + param_name) not in state_dict:\n                state_dict[prefix + param_name] = self.state_dict()[param_name]\n\n        if len(state_dict[\"decoder.embed_image_positions.weight\"]) < len(self.state_dict()[\"embed_image_positions.weight\"]):\n            num_posids_to_add = len(self.state_dict()[\"embed_image_positions.weight\"]) - len(state_dict[\"decoder.embed_image_positions.weight\"])\n            embed_dim = state_dict[\"decoder.embed_image_positions.weight\"].size(1)\n            new_pos_embed_to_add = torch.zeros(num_posids_to_add, embed_dim)\n            nn.init.normal_(new_pos_embed_to_add, mean=0, std=embed_dim ** -0.5)\n            new_pos_embed_to_add = new_pos_embed_to_add.to(\n                dtype=state_dict[\"decoder.embed_image_positions.weight\"].dtype,\n            )\n            state_dict[\"decoder.embed_image_positions.weight\"] = torch.cat(\n                [state_dict[\"decoder.embed_image_positions.weight\"], new_pos_embed_to_add]\n            )\n        return state_dict\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx=None, zero_init=False):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n    if padding_idx is not None:\n        nn.init.constant_(m.weight[padding_idx], 0)\n    if zero_init:\n        nn.init.constant_(m.weight, 0)\n    return m\n\n\ndef Linear(in_features, out_features, bias=True):\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.0)\n    return m\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:35:07.388042Z","iopub.execute_input":"2024-10-12T23:35:07.388353Z","iopub.status.idle":"2024-10-12T23:35:07.587013Z","shell.execute_reply.started":"2024-10-12T23:35:07.388304Z","shell.execute_reply":"2024-10-12T23:35:07.586010Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n@register_model(\"ofa\")\nclass OFAModel(FairseqEncoderDecoderModel):\n    __jit_unused_properties__ = [\"supported_targets\"]\n\n    def __init__(self, args, encoder, decoder):\n        super().__init__(encoder, decoder)\n\n        # We follow BERT's random weight initialization\n        self.apply(init_bert_params)\n\n        self.classification_heads = nn.ModuleDict()\n        if hasattr(self.encoder, \"dictionary\"):\n            self.eos: int = self.encoder.dictionary.eos()\n\n    @staticmethod\n    def add_args(parser):\n        super(OFAModel, OFAModel).add_args(parser)\n        parser.add_argument(\n            \"--pooler-dropout\",\n            type=float,\n            metavar=\"D\",\n            help=\"dropout probability in the masked_lm pooler layers\",\n        )\n        parser.add_argument(\n            \"--pooler-classifier\",\n            type=str,\n            choices=['mlp', 'linear'],\n            help=\"type of pooler classifier\",\n        )\n        parser.add_argument(\n            \"--pooler-activation-fn\",\n            choices=utils.get_available_activation_fns(),\n            help=\"activation function to use for pooler layer\",\n        )\n        parser.add_argument(\n            \"--spectral-norm-classification-head\",\n            action=\"store_true\",\n            help=\"Apply spectral normalization on the classification head\",\n        )\n\n    @property\n    def supported_targets(self):\n        return {\"self\"}\n\n    def forward(\n        self,\n        src_tokens,\n        src_lengths,\n        prev_output_tokens,\n        patch_images: Optional[torch.Tensor] = None,\n        patch_images_2: Optional[torch.Tensor] = None,\n        patch_masks: Optional[torch.Tensor] = None,\n        code_masks: Optional[torch.Tensor] = None,\n        sample_patch_num: Optional[int] = None,\n        features_only: bool = False,\n        classification_head_name: Optional[str] = None,\n        token_embeddings: Optional[torch.Tensor] = None,\n        return_all_hiddens: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n    ):\n        if classification_head_name is not None:\n            features_only = True\n\n        encoder_out = self.encoder(\n            src_tokens,\n            src_lengths=src_lengths,\n            patch_images=patch_images,\n            patch_masks=patch_masks,\n            patch_images_2=patch_images_2,\n            token_embeddings=token_embeddings,\n            return_all_hiddens=return_all_hiddens,\n            sample_patch_num=sample_patch_num\n        )\n        x, extra = self.decoder(\n            prev_output_tokens,\n            code_masks=code_masks,\n            encoder_out=encoder_out,\n            features_only=features_only,\n            alignment_layer=alignment_layer,\n            alignment_heads=alignment_heads,\n            src_lengths=src_lengths,\n            return_all_hiddens=return_all_hiddens,\n        )\n\n        pad = self.encoder.padding_idx\n        if classification_head_name is not None:\n            prev_lengths = prev_output_tokens.ne(pad).sum(1)\n            gather_index = prev_lengths[:, None, None].expand(x.size(0), 1, x.size(2)) - 1\n            sentence_representation = x.gather(1, gather_index).squeeze()\n            if self.classification_heads[classification_head_name].use_two_images:\n                hidden_size = sentence_representation.size(1)\n                sentence_representation = sentence_representation.view(-1, hidden_size * 2)\n            for k, head in self.classification_heads.items():\n                # for torch script only supports iteration\n                if k == classification_head_name:\n                    x = head(sentence_representation)\n                    break\n\n        return x, extra\n\n    @classmethod\n    def build_embedding(cls, args, dictionary, embed_dim, path=None):\n        num_embeddings = len(dictionary)\n        padding_idx = dictionary.pad()\n\n        args.vocab_size = num_embeddings\n        emb = Embedding(num_embeddings, embed_dim, padding_idx)\n        # if provided, load from preloaded dictionaries\n        if path:\n            embed_dict = utils.parse_embedding(path)\n            utils.load_embedding(embed_dict, dictionary, emb)\n        return emb\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        return TransformerEncoder(args, src_dict, embed_tokens)\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        return TransformerDecoder(\n            args,\n            tgt_dict,\n            embed_tokens,\n            no_encoder_attn=getattr(args, \"no_cross_attention\", False),\n        )\n\n\n\n    \n    def register_embedding_tokens(self, ans2label_dict, src_dict, bpe):\n        \"\"\"Register embedding tokens\"\"\"\n        logger.info(\"Registering embedding tokens\")\n        self.ans_tensor_list = []\n        for i in range(len(ans2label_dict)):\n            ans = src_dict[-len(ans2label_dict)+i]\n            ans = ans[5:-1].replace('_', ' ')\n            ans_tensor = src_dict.encode_line(\n                line=bpe.encode(' {}'.format(ans.lower())),\n                add_if_not_exist=False,\n                append_eos=False\n            ).long()\n            self.ans_tensor_list.append(ans_tensor)\n\n    def register_classification_head(\n        self, name, num_classes=None, inner_dim=None, use_two_images=False, **kwargs\n    ):\n        \"\"\"Register a classification head.\"\"\"\n        logger.info(\"Registering classification head: {0}\".format(name))\n        if name in self.classification_heads:\n            prev_num_classes = self.classification_heads[name].out_proj.out_features\n            prev_inner_dim = self.classification_heads[name].dense.out_features\n            if num_classes != prev_num_classes or inner_dim != prev_inner_dim:\n                logger.warning(\n                    're-registering head \"{}\" with num_classes {} (prev: {}) '\n                    \"and inner_dim {} (prev: {})\".format(\n                        name, num_classes, prev_num_classes, inner_dim, prev_inner_dim\n                    )\n                )\n        self.classification_heads[name] = OFAClassificationHead(\n            input_dim=self.args.encoder_embed_dim,\n            inner_dim=inner_dim or self.args.encoder_embed_dim,\n            num_classes=num_classes,\n            activation_fn=self.args.pooler_activation_fn,\n            pooler_dropout=self.args.pooler_dropout,\n            pooler_classifier=self.args.pooler_classifier,\n            use_two_images=use_two_images,\n            do_spectral_norm=getattr(\n                self.args, \"spectral_norm_classification_head\", False\n            ),\n        )\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        super().upgrade_state_dict_named(state_dict, name)\n\n        prefix = name + \".\" if name != \"\" else \"\"\n        current_head_names = (\n            []\n            if not hasattr(self, \"classification_heads\")\n            else self.classification_heads.keys()\n        )\n\n        # Handle new classification heads present in the state dict.\n        keys_to_delete = []\n        for k in state_dict.keys():\n            if not k.startswith(prefix + \"classification_heads.\"):\n                continue\n\n            head_name = k[len(prefix + \"classification_heads.\") :].split(\".\")[0]\n            num_classes = state_dict[\n                prefix + \"classification_heads.\" + head_name + \".out_proj.weight\"\n            ].size(0)\n            inner_dim = state_dict[\n                prefix + \"classification_heads.\" + head_name + \".dense.weight\"\n            ].size(0)\n\n            if getattr(self.args, \"load_checkpoint_heads\", False):\n                if head_name not in current_head_names:\n                    self.register_classification_head(head_name, num_classes, inner_dim)\n            else:\n                if head_name not in current_head_names:\n                    logger.warning(\n                        \"deleting classification head ({}) from checkpoint \"\n                        \"not present in current model: {}\".format(head_name, k)\n                    )\n                    keys_to_delete.append(k)\n                elif (\n                    num_classes\n                    != self.classification_heads[head_name].out_proj.out_features\n                    or inner_dim\n                    != self.classification_heads[head_name].dense.out_features\n                ):\n                    logger.warning(\n                        \"deleting classification head ({}) from checkpoint \"\n                        \"with different dimensions than current model: {}\".format(\n                            head_name, k\n                        )\n                    )\n                    keys_to_delete.append(k)\n        for k in keys_to_delete:\n            del state_dict[k]\n\n        def truncate_emb(key):\n            if key in state_dict:\n                state_dict[key] = state_dict[key][:-1, :]\n\n        # When finetuning on translation task, remove last row of\n        # embedding matrix that corresponds to mask_idx token.\n        loaded_dict_size = state_dict[\"encoder.embed_tokens.weight\"].size(0)\n        if (\n            loaded_dict_size == len(self.encoder.dictionary) + 1\n            and \"<mask>\" not in self.encoder.dictionary\n        ):\n            truncate_emb(\"encoder.embed_tokens.weight\")\n            truncate_emb(\"decoder.embed_tokens.weight\")\n            truncate_emb(\"encoder.output_projection.weight\")\n            truncate_emb(\"decoder.output_projection.weight\")\n\n        if loaded_dict_size < len(self.encoder.dictionary):\n            num_langids_to_add = len(self.encoder.dictionary) - loaded_dict_size\n            embed_dim = state_dict[\"encoder.embed_tokens.weight\"].size(1)\n\n            new_lang_embed_to_add = torch.zeros(num_langids_to_add, embed_dim)\n            if getattr(self, \"ans_tensor_list\", None):\n                assert len(new_lang_embed_to_add) == len(self.ans_tensor_list)\n                for i, ans_tensor in enumerate(self.ans_tensor_list):\n                    ans_embed = F.embedding(ans_tensor, state_dict[\"encoder.embed_tokens.weight\"])\n                    ans_embed = ans_embed.sum(0) / ans_embed.size(0)\n                    new_lang_embed_to_add[i] = ans_embed\n            else:\n                nn.init.normal_(new_lang_embed_to_add, mean=0, std=embed_dim ** -0.5)\n            new_lang_embed_to_add = new_lang_embed_to_add.to(\n                dtype=state_dict[\"encoder.embed_tokens.weight\"].dtype,\n            )\n\n            state_dict[\"encoder.embed_tokens.weight\"] = torch.cat(\n                [state_dict[\"encoder.embed_tokens.weight\"], new_lang_embed_to_add]\n            )\n            state_dict[\"decoder.embed_tokens.weight\"] = torch.cat(\n                [state_dict[\"decoder.embed_tokens.weight\"], new_lang_embed_to_add]\n            )\n            state_dict[\"decoder.output_projection.weight\"] = torch.cat(\n                [state_dict[\"decoder.output_projection.weight\"], new_lang_embed_to_add]\n            )\n\n        # Copy any newly-added classification heads into the state dict\n        # with their current weights.\n        if hasattr(self, \"classification_heads\"):\n            cur_state = self.classification_heads.state_dict()\n            for k, v in cur_state.items():\n                if prefix + \"classification_heads.\" + k not in state_dict:\n                    logger.info(\"Overwriting \" + prefix + \"classification_heads.\" + k)\n                    state_dict[prefix + \"classification_heads.\" + k] = v\n\n\nclass OFAClassificationHead(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n\n    def __init__(\n        self,\n        input_dim,\n        inner_dim,\n        num_classes,\n        activation_fn,\n        pooler_dropout,\n        pooler_classifier,\n        use_two_images=False,\n        do_spectral_norm=False,\n    ):\n        super().__init__()\n        self.pooler_classifier = pooler_classifier\n        self.use_two_images = use_two_images\n        input_dim = input_dim * 2 if use_two_images else input_dim\n        if pooler_classifier == \"mlp\":\n            self.dense = nn.Linear(input_dim, inner_dim)\n            self.activation_fn = utils.get_activation_fn(activation_fn)\n            self.dropout = nn.Dropout(p=pooler_dropout)\n            self.out_proj = nn.Linear(inner_dim, num_classes)\n        elif pooler_classifier == \"linear\":\n            self.dropout = nn.Dropout(p=pooler_dropout)\n            self.out_proj = nn.Linear(input_dim, num_classes)\n        else:\n            raise NotImplementedError\n\n        if do_spectral_norm:\n            self.out_proj = torch.nn.utils.spectral_norm(self.out_proj)\n\n    def forward(self, features, **kwargs):\n        if self.pooler_classifier == 'mlp':\n            x = features\n            x = self.dropout(x)\n            x = self.dense(x)\n            x = self.activation_fn(x)\n            x = self.dropout(x)\n            x = self.out_proj(x)\n        elif self.pooler_classifier == 'linear':\n            x = features\n            x = self.dropout(x)\n            x = self.out_proj(x)\n        else:\n            raise NotImplementedError\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:35:07.588708Z","iopub.execute_input":"2024-10-12T23:35:07.589112Z","iopub.status.idle":"2024-10-12T23:35:07.636707Z","shell.execute_reply.started":"2024-10-12T23:35:07.589066Z","shell.execute_reply":"2024-10-12T23:35:07.635722Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Cell 4: Define Model Arguments Class\n\nclass Args:\n    def __init__(self):\n        # Activation Function and Dropout\n        self.activation_fn = 'relu'\n        self.dropout = 0.1\n        self.attention_dropout = 0.1\n        self.activation_dropout = 0.1\n        \n        # Encoder Embeddings\n        self.encoder_embed_path = None\n        self.encoder_embed_dim = 512\n        self.encoder_ffn_embed_dim = 2048\n        self.encoder_layers = 6\n        self.encoder_attention_heads = 8\n        self.encoder_normalize_before = False\n        self.encoder_learned_pos = False\n        self.encoder_prompt = False\n        self.encoder_prompt_type = 'prefix'\n        self.encoder_prompt_projection = False\n        self.encoder_prompt_length = 100\n        self.encoder_prompt_dim = 1024\n        self.encoder_layerdrop = 0.0\n        self.encoder_layers_to_keep = None\n        \n        # Decoder Embeddings\n        self.decoder_embed_path = None\n        self.decoder_embed_dim = 512\n        self.decoder_ffn_embed_dim = 2048\n        self.decoder_layers = 6\n        self.decoder_attention_heads = 8\n        self.decoder_normalize_before = False\n        self.decoder_learned_pos = False\n        self.decoder_output_dim = 512\n        self.decoder_prompt = False\n        self.decoder_prompt_type = 'prefix'\n        self.decoder_prompt_length = 100\n        self.decoder_prompt_projection = False\n        self.decoder_prompt_dim = 1024\n        self.decoder_layerdrop = 0.0\n        self.decoder_layers_to_keep = None\n        \n        # Embedding Sharing and Positional Embeddings\n        self.share_decoder_input_output_embed = False\n        self.share_all_embeddings = False\n        self.no_token_positional_embeddings = False\n        self.layernorm_embedding = False\n        self.no_scale_embedding = False\n        self.entangle_position_embedding = False\n        self.disable_entangle = False\n        \n        # Quantization Noise and Checkpointing\n        self.quant_noise_pq = 0.0\n        self.quant_noise_pq_block_size = 8\n        self.quant_noise_scalar = 0.0\n        self.checkpoint_activations = False\n        self.offload_activations = False\n        \n        # Adapter Settings\n        self.adapter = False\n        self.adapter_dim = 64\n        \n        # ResNet Settings\n        self.resnet_type = 'resnet50'\n        self.resnet_drop_path_rate = 0.0\n        self.resnet_model_path = None\n        self.freeze_resnet = False\n        \n        # Batch Normalization and Scaling\n        self.sync_bn = False\n        self.scale_attn = False\n        self.scale_fc = False\n        self.scale_heads = False\n        self.scale_resids = False\n        \n        # LayerDrop and Pruning\n        self.encoder_layerdrop = 0.0\n        self.decoder_layerdrop = 0.0\n        self.encoder_layers_to_keep = None\n        self.decoder_layers_to_keep = None\n        \n        # Bucket Sizes and Attention Scaling\n        self.token_bucket_size = 5\n        self.image_bucket_size = 10\n        self.attn_scale_factor = 1.0\n        \n        # Code Image Size\n        self.code_image_size = 224\n        \n        # Miscellaneous\n        self.min_params_to_wrap = 1e8\n        \n        # Cross-Attention Settings\n        self.no_cross_attention = False\n        self.cross_self_attention = False\n\n        self.max_source_positions = 1024\n        self.adaptive_input = False\n        self.encoder_drop_path_rate =0\n        self.decoder_drop_path_rate = 0\n        self.patch_image_size=256\n        self.sample_patch_num=196\n        self.max_image_size=512\n        self.orig_patch_image_size =256\n        self.max_target_positions = 1024\n        self.adaptive_softmax_cutoff =None\n# Instantiate the arguments\nargs = Args()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:35:07.638267Z","iopub.execute_input":"2024-10-12T23:35:07.639005Z","iopub.status.idle":"2024-10-12T23:35:07.654618Z","shell.execute_reply.started":"2024-10-12T23:35:07.638958Z","shell.execute_reply":"2024-10-12T23:35:07.653764Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Cell 5: Create Dictionaries\nfrom fairseq.data import Dictionary\n\n# Create source and target dictionaries\nsrc_dict = Dictionary()\ntgt_dict = Dictionary()\n\n# Add special tokens first\nspecial_tokens = ['<pad>', '<eos>', '<unk>']\nfor token in special_tokens:\n    src_dict.add_symbol(token)\n    tgt_dict.add_symbol(token)\n\n# Add additional tokens\nadditional_tokens = ['hello', 'world', 'I', 'am', 'a', 'student', 'teacher', 'machine', 'learning', 'model']\nfor token in additional_tokens:\n    src_dict.add_symbol(token)\n    tgt_dict.add_symbol(token)\n\n\nencoder_embed = nn.Embedding(\n    num_embeddings=len(src_dict),\n    embedding_dim=args.encoder_embed_dim,\n    padding_idx=src_dict.pad()  # Set padding_idx\n)\n\ndecoder_embed = nn.Embedding(\n    num_embeddings=len(tgt_dict),\n    embedding_dim=args.decoder_embed_dim,\n    padding_idx=tgt_dict.pad()  # Set padding_idx\n)\n\n\nencoder=TransformerEncoder(args, dictionary=src_dict, embed_tokens=encoder_embed)\ndecoder=TransformerDecoder(args, dictionary=src_dict, embed_tokens=encoder_embed)\n\nmodel=OFAModel(args,encoder,decoder)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:35:07.657549Z","iopub.execute_input":"2024-10-12T23:35:07.658337Z","iopub.status.idle":"2024-10-12T23:35:08.972126Z","shell.execute_reply.started":"2024-10-12T23:35:07.658292Z","shell.execute_reply":"2024-10-12T23:35:08.971166Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3609.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"OFAModel(\n  (encoder): TransformerEncoder(\n    (encoder_dropout): Dropout(p=0.2, inplace=False)\n    (dropout_module): FairseqDropout()\n    (embed_tokens): Embedding(15, 512, padding_idx=1)\n    (embed_images): ResNet(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (drop_path): Identity()\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (drop_path): Identity()\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (drop_path): Identity()\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (drop_path): Identity()\n        )\n      )\n    )\n    (image_proj): Linear(in_features=1024, out_features=512, bias=True)\n    (embed_positions): Embedding(1026, 512)\n    (embed_image_positions): Embedding(101, 512)\n    (pos_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (image_pos_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (pos_q_linear): Linear(in_features=512, out_features=512, bias=True)\n    (pos_k_linear): Linear(in_features=512, out_features=512, bias=True)\n    (layers): ModuleList(\n      (0-5): 6 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (dropout_module): FairseqDropout()\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout_module): FairseqDropout()\n        (activation_dropout_module): FairseqDropout()\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (drop_path): Identity()\n      )\n    )\n    (token_rel_pos_table_list): ModuleList(\n      (0-5): 6 x Embedding(9, 8)\n    )\n    (image_rel_pos_table_list): ModuleList(\n      (0-5): 6 x Embedding(364, 8)\n    )\n  )\n  (decoder): TransformerDecoder(\n    (dropout_module): FairseqDropout()\n    (embed_tokens): Embedding(15, 512, padding_idx=1)\n    (embed_positions): Embedding(1026, 512)\n    (embed_image_positions): Embedding(101, 512)\n    (pos_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (image_pos_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (self_pos_q_linear): Linear(in_features=512, out_features=512, bias=True)\n    (self_pos_k_linear): Linear(in_features=512, out_features=512, bias=True)\n    (cross_pos_q_linear): Linear(in_features=512, out_features=512, bias=True)\n    (cross_pos_k_linear): Linear(in_features=512, out_features=512, bias=True)\n    (layers): ModuleList(\n      (0-5): 6 x TransformerDecoderLayer(\n        (dropout_module): FairseqDropout()\n        (self_attn): MultiheadAttention(\n          (dropout_module): FairseqDropout()\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (activation_dropout_module): FairseqDropout()\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): MultiheadAttention(\n          (dropout_module): FairseqDropout()\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (drop_path): Identity()\n      )\n    )\n    (output_projection): Linear(in_features=512, out_features=15, bias=False)\n    (token_rel_pos_table_list): ModuleList(\n      (0-5): 6 x Embedding(9, 8)\n    )\n    (image_rel_pos_table_list): ModuleList(\n      (0-5): 6 x Embedding(364, 8)\n    )\n  )\n  (classification_heads): ModuleDict()\n)\n","output_type":"stream"}]}]}